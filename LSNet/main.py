import torch
from model_pytorch import Model
from torchvision import models
import argparse
from sklearn.metrics import confusion_matrix
from models import *
from models.mobile_vit import MobileViT
from models.sparse_mobile_vit import SparseMobileViT
from utils import progress_bar
from sklearn.metrics import roc_auc_score, roc_curve, auc
import os
from torch import optim
import torch.nn.functional as F
from torch import nn
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import matplotlib.pyplot as plt
import torchvision.utils
import numpy as np
import random
from PIL import Image
import PIL.ImageOps
from torch.utils.data.dataset import ConcatDataset
from utilsfile.mask_utils import create_subgraph_mask2coords, create_rectangle_mask, create_rectangle_mask2coords, \
    create_bond_mask2coords
from utilsfile.public_utils import setup_device
from skimage.color import rgb2gray
from skimage.restoration import denoise_tv_chambolle
from skimage.feature import corner_peaks
from utilsfile.harris import CornerDetection
import time
from warmup_scheduler import GradualWarmupScheduler
import copy
from medical_image_loader import PairedMedicalImageDataset
import seaborn as sns
import pandas as pd
from itertools import cycle
import warnings

warnings.filterwarnings('ignore')


# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°ï¼ˆä¸resnet_3t.pyä¿æŒä¸€è‡´ï¼‰
def calculate_all_metrics(all_labels, all_preds, all_probs, class_names):
    from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_curve, auc, \
        precision_recall_curve

    accuracy = accuracy_score(all_labels, all_preds)

    if len(class_names) == 2:
        sensitivity = recall_score(all_labels, all_preds, pos_label=1, zero_division=0)
        specificity = recall_score(all_labels, all_preds, pos_label=0, zero_division=0)
        precision = precision_score(all_labels, all_preds, pos_label=1, zero_division=0)
        f1 = f1_score(all_labels, all_preds, pos_label=1, zero_division=0)

        fpr, tpr, _ = roc_curve(all_labels, all_probs[:, 1])
        roc_auc = auc(fpr, tpr)

        precision_vals, recall_vals, _ = precision_recall_curve(all_labels, all_probs[:, 1])
        pr_auc = auc(recall_vals, precision_vals)

        class_metrics = {}
        for i, class_name in enumerate(class_names):
            class_recall = recall_score(all_labels, all_preds, labels=[i], average=None)[0] if i in all_labels else 0
            class_precision = precision_score(all_labels, all_preds, labels=[i], average=None)[
                0] if i in all_preds else 0
            class_f1 = f1_score(all_labels, all_preds, labels=[i], average=None)[0] if (
                    i in all_labels and i in all_preds) else 0

            class_metrics[class_name] = {
                'recall': class_recall,
                'sensitivity': class_recall if i == 1 else None,
                'specificity': class_recall if i == 0 else None,
                'precision': class_precision,
                'f1_score': class_f1
            }

        metrics_dict = {
            'accuracy': accuracy,
            'sensitivity': sensitivity,
            'specificity': specificity,
            'precision': precision,
            'f1_score': f1,
            'roc_auc': roc_auc,
            'pr_auc': pr_auc,
            'class_metrics': class_metrics,
            'confusion_matrix': confusion_matrix(all_labels, all_preds),
            'fpr': fpr,
            'tpr': tpr
        }
    else:
        # å¤šç±»åˆ«å¤„ç†
        from sklearn.metrics import classification_report
        report = classification_report(all_labels, all_preds, output_dict=True)

        # è®¡ç®—å¤šç±»åˆ«ROC AUC
        from sklearn.preprocessing import label_binarize
        y_test_bin = label_binarize(all_labels, classes=range(len(class_names)))

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ROCæ›²çº¿
        fpr = {}
        tpr = {}
        roc_auc = {}
        for i in range(len(class_names)):
            if len(np.unique(all_probs[:, i])) > 1:
                fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], all_probs[:, i])
                roc_auc[i] = auc(fpr[i], tpr[i])

        # è®¡ç®—å®å¹³å‡ROC
        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(class_names))]))
        mean_tpr = np.zeros_like(all_fpr)
        for i in range(len(class_names)):
            if i in tpr:
                mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
        mean_tpr /= len(class_names)

        metrics_dict = {
            'accuracy': accuracy,
            'classification_report': report,
            'roc_auc_macro': auc(all_fpr, mean_tpr),
            'fpr': all_fpr,
            'tpr': mean_tpr,
            'roc_auc_per_class': roc_auc
        }

    return metrics_dict


# è®¡ç®—å¹³å‡ROCæ›²çº¿å’Œæ ‡å‡†å·®åŒºé—´
def compute_mean_roc_curves(all_roc_data, n_points=100):
    """
    è®¡ç®—ä¸‰æ¬¡å®éªŒçš„å¹³å‡ROCæ›²çº¿å’Œæ ‡å‡†å·®åŒºé—´
    """
    mean_fpr = np.linspace(0, 1, n_points)

    tprs_interp = []
    for fpr, tpr in all_roc_data:
        if len(np.unique(fpr)) < 2:
            continue
        tpr_interp = np.interp(mean_fpr, fpr, tpr)
        tpr_interp[0] = 0.0
        tprs_interp.append(tpr_interp)

    if len(tprs_interp) == 0:
        return mean_fpr, np.zeros_like(mean_fpr), np.zeros_like(mean_fpr), []

    tprs_array = np.array(tprs_interp)
    mean_tpr = np.mean(tprs_array, axis=0)
    std_tpr = np.std(tprs_array, axis=0)

    return mean_fpr, mean_tpr, std_tpr, tprs_interp


# ç»˜åˆ¶å¹³å‡ROCæ›²çº¿Â±æ ‡å‡†å·®åŒºé—´
def plot_mean_roc_curve_with_std(all_roc_data, all_auc_values, model_name="Model", save_path='./results'):
    """
    Plot mean ROC curve with standard deviation interval.
    """
    os.makedirs(save_path, exist_ok=True)

    mean_fpr, mean_tpr, std_tpr, tprs_interp = compute_mean_roc_curves(all_roc_data)

    mean_auc = np.mean(all_auc_values)
    std_auc = np.std(all_auc_values)

    plt.figure(figsize=(10, 8))

    colors = plt.cm.Set1(np.linspace(0, 1, len(all_roc_data)))
    for i, (fpr, tpr) in enumerate(all_roc_data):
        auc_value = auc(fpr, tpr)
        plt.plot(fpr, tpr, color=colors[i], alpha=0.3, lw=2,
                 label=f'Experiment {i + 1} (AUC = {auc_value:.3f})' if i < 3 else f'Experiment {i + 1}')

    plt.plot(mean_fpr, mean_tpr, color='b', lw=3,
             label=f'Mean ROC (AUC = {mean_auc:.3f} Â± {std_auc:.3f})')

    tpr_upper = np.minimum(mean_tpr + std_tpr, 1)
    tpr_lower = np.maximum(mean_tpr - std_tpr, 0)
    plt.fill_between(mean_fpr, tpr_lower, tpr_upper, color='grey', alpha=0.3,
                     label='Â±1 Standard Deviation')

    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Guess')

    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=24)
    plt.ylabel('True Positive Rate', fontsize=24)
    plt.title(f'Mean ROC Curve', fontsize=26)
    plt.legend(loc="lower right", fontsize=20)
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(os.path.join(save_path, f'{model_name}_mean_roc_curve.png'), dpi=150)
    plt.savefig(os.path.join(save_path, f'{model_name}_mean_roc_curve.pdf'))
    plt.show()

    print(f"\n{model_name} ROC Curve Statistics:")
    print(f"Mean AUC: {mean_auc:.4f} Â± {std_auc:.4f}")
    print(f"AUC Values from Three Experiments: {[f'{auc_val:.4f}' for auc_val in all_auc_values]}")
    print(f"AUC Range: {min(all_auc_values):.4f} - {max(all_auc_values):.4f}")

    return mean_auc, std_auc


# ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æ›²çº¿çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®
def plot_mean_training_curves(all_train_histories, save_path='./results'):
    """
    Plot mean training and validation curves with standard deviation.
    """
    os.makedirs(save_path, exist_ok=True)

    # æå–æ‰€æœ‰å†å²æ•°æ®
    all_train_losses = [h['train_losses'] for h in all_train_histories]
    all_val_losses = [h['val_losses'] for h in all_train_histories]
    all_train_accs = [h['train_accs'] for h in all_train_histories]
    all_val_accs = [h['val_accs'] for h in all_train_histories]

    # ç¡®ä¿æ‰€æœ‰å†å²æ•°æ®é•¿åº¦ç›¸åŒ
    min_length = min(len(losses) for losses in all_train_losses)
    all_train_losses = [losses[:min_length] for losses in all_train_losses]
    all_val_losses = [losses[:min_length] for losses in all_val_losses]
    all_train_accs = [accs[:min_length] for accs in all_train_accs]
    all_val_accs = [accs[:min_length] for accs in all_val_accs]

    epochs = range(1, min_length + 1)

    # è½¬æ¢ä¸ºnumpyæ•°ç»„è¿›è¡Œç»Ÿè®¡è®¡ç®—
    train_losses_array = np.array(all_train_losses)
    val_losses_array = np.array(all_val_losses)
    train_accs_array = np.array(all_train_accs)
    val_accs_array = np.array(all_val_accs)

    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    mean_train_losses = np.mean(train_losses_array, axis=0)
    std_train_losses = np.std(train_losses_array, axis=0)

    mean_val_losses = np.mean(val_losses_array, axis=0)
    std_val_losses = np.std(val_losses_array, axis=0)

    mean_train_accs = np.mean(train_accs_array, axis=0)
    std_train_accs = np.std(train_accs_array, axis=0)

    mean_val_accs = np.mean(val_accs_array, axis=0)
    std_val_accs = np.std(val_accs_array, axis=0)

    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    axes[0, 0].plot(epochs, mean_train_losses, 'b-', label='Training Loss (Mean)', linewidth=2)
    axes[0, 0].fill_between(epochs,
                            mean_train_losses - std_train_losses,
                            mean_train_losses + std_train_losses,
                            color='blue', alpha=0.2)

    axes[0, 0].plot(epochs, mean_val_losses, 'r-', label='Validation Loss (Mean)', linewidth=2)
    axes[0, 0].fill_between(epochs,
                            mean_val_losses - std_val_losses,
                            mean_val_losses + std_val_losses,
                            color='red', alpha=0.2)

    axes[0, 0].set_title('Training and Validation Loss (Mean Â± Std)', fontsize=14)
    axes[0, 0].set_xlabel('Epoch', fontsize=12)
    axes[0, 0].set_ylabel('Loss', fontsize=12)
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿
    axes[0, 1].plot(epochs, mean_train_accs, 'b-', label='Training Accuracy (Mean)', linewidth=2)
    axes[0, 1].fill_between(epochs,
                            mean_train_accs - std_train_accs,
                            mean_train_accs + std_train_accs,
                            color='blue', alpha=0.2)

    axes[0, 1].plot(epochs, mean_val_accs, 'r-', label='Validation Accuracy (Mean)', linewidth=2)
    axes[0, 1].fill_between(epochs,
                            mean_val_accs - std_val_accs,
                            mean_val_accs + std_val_accs,
                            color='red', alpha=0.2)

    axes[0, 1].set_title('Training and Validation Accuracy (Mean Â± Std)', fontsize=14)
    axes[0, 1].set_xlabel('Epoch', fontsize=12)
    axes[0, 1].set_ylabel('Accuracy', fontsize=12)
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # ç»˜åˆ¶æœ€ä½³æ€§èƒ½æŒ‡æ ‡ç®±çº¿å›¾
    all_best_accs = [h['best_acc'] for h in all_train_histories]

    data_to_plot = [all_best_accs]
    bp = axes[1, 0].boxplot(data_to_plot, patch_artist=True,
                            labels=['Best Accuracy'])

    # è®¾ç½®ç®±çº¿å›¾é¢œè‰²
    colors = ['lightblue']
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)

    # æ·»åŠ å•ä¸ªæ•°æ®ç‚¹
    for i, data in enumerate(data_to_plot, 1):
        y = data
        x = np.random.normal(i, 0.04, size=len(y))
        axes[1, 0].plot(x, y, 'r.', alpha=0.6)

    axes[1, 0].set_title('Distribution of Best Accuracy Across Three Experiments', fontsize=14)
    axes[1, 0].set_ylabel('Accuracy', fontsize=12)
    axes[1, 0].grid(True, alpha=0.3, axis='y')

    # ç»˜åˆ¶æœ€ä½³éªŒè¯æ•æ„Ÿåº¦ç®±çº¿å›¾
    all_best_sensitivities = [h['best_sensitivity'] for h in all_train_histories]
    data_to_plot_sen = [all_best_sensitivities]
    bp_sen = axes[1, 1].boxplot(data_to_plot_sen, patch_artist=True,
                                labels=['Best Sensitivity'])

    colors_sen = ['lightgreen']
    for patch, color in zip(bp_sen['boxes'], colors_sen):
        patch.set_facecolor(color)

    for i, data in enumerate(data_to_plot_sen, 1):
        y = data
        x = np.random.normal(i, 0.04, size=len(y))
        axes[1, 1].plot(x, y, 'r.', alpha=0.6)

    axes[1, 1].set_title('Distribution of Best Sensitivity Across Three Experiments', fontsize=14)
    axes[1, 1].set_ylabel('Sensitivity', fontsize=12)
    axes[1, 1].grid(True, alpha=0.3, axis='y')

    plt.suptitle('Training Process Statistics from Three Independent Experiments', fontsize=16)
    plt.tight_layout()
    plt.savefig(os.path.join(save_path, 'mean_training_curves.png'), dpi=150)
    plt.savefig(os.path.join(save_path, 'mean_training_curves.pdf'))
    plt.show()


# æ·»åŠ FLOPsè®¡ç®—å·¥å…·
try:
    from thop import profile

    THOP_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: æœªæ‰¾åˆ°thopåº“ï¼Œæ— æ³•è®¡ç®—FLOPsã€‚è¯·ä½¿ç”¨ 'pip install thop' å®‰è£…")
    THOP_AVAILABLE = False

try:
    from fvcore.nn import FlopCountAnalysis, parameter_count_table

    FVCORE_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: æœªæ‰¾åˆ°fvcoreåº“ï¼Œæ— æ³•ä½¿ç”¨FlopCountAnalysisã€‚è¯·ä½¿ç”¨ 'pip install fvcore' å®‰è£…")
    FVCORE_AVAILABLE = False

# parsers
parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')
parser.add_argument('--lr', default=5e-4, type=float, help='learning rate')  # resnets.. 1e-3, Vit..1e-4?
parser.add_argument('--opt', default="adam")
parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')
parser.add_argument('--aug', action='store_true', help='add image augumentations')
parser.add_argument('--mixup', action='store_true', help='add mixup augumentations')
parser.add_argument('--net', default='vit')
parser.add_argument('--bs', type=int, default='16')  # 64
parser.add_argument('--weight_decay', default=1e-6, type=float, help='SGD weight decay')
parser.add_argument('--data_address', default='../../data/Pretraining/', type=str)
parser.add_argument('--n_epochs', type=int, default='20')
parser.add_argument('--dim', type=int, default='256')
parser.add_argument('--sparsedim', type=int, default='128')
parser.add_argument('--imagesize', type=int, default='320')  # 288
parser.add_argument('--num_classes', type=int, default=2)
parser.add_argument('--tau', type=float, default=0.99)
parser.add_argument('--cos', default='True', action='store_true', help='Train with cosine annealing scheduling')
args = parser.parse_args()

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.backends.cudnn.benchmark = True
size = int(args.imagesize)

dims = [128, 160, 192]
channels = [16, 32, 64, 64, 128, 128, 160, 160, 192, 192, 384]
heads = 4
vit = MobileViT((size, size), dims, channels, heads, num_classes=args.num_classes).to(device)

dims = [40, 48, 56]
channels = [16, 16, 24, 24, 32, 32, 48, 48, 64, 64, 128]
vit_sparse = SparseMobileViT((size, size), dims, channels, heads, num_classes=args.num_classes, expansion=2).to(device)


def calculate_flops(model, input_sizes=[(1, 3, 256, 256)]):
    """
    è®¡ç®—æ¨¡å‹çš„FLOPså’Œå‚æ•°é‡ï¼Œæ”¯æŒå¤šè¾“å…¥åœºæ™¯
    """
    model.eval()

    dummy_inputs = tuple(
        torch.randn(size).to(device)
        for size in input_sizes
    )

    print("=" * 60)
    print(f"è®¡ç®—æ¨¡å‹çš„FLOPså’Œå‚æ•°é‡ï¼ˆ{len(input_sizes)}ä¸ªè¾“å…¥ï¼‰")
    print("=" * 60)

    if THOP_AVAILABLE:
        try:
            flops, params = profile(model, inputs=dummy_inputs, verbose=False)
            print(f"[thop] å‚æ•°é‡: {params:,} ({params / 1e6:.3f}M)")
            print(f"[thop] FLOPs: {flops:,} ({flops / 1e9:.3f}G)")
        except Exception as e:
            print(f"thopè®¡ç®—å¤±è´¥: {e}")

    if FVCORE_AVAILABLE:
        try:
            flops_analyzer = FlopCountAnalysis(model, dummy_inputs)
            flops_fvcore = flops_analyzer.total()
            print(f"[fvcore] FLOPs: {flops_fvcore:,} ({flops_fvcore / 1e9:.2f}G)")
            print(parameter_count_table(model))
        except Exception as e:
            print(f"fvcoreè®¡ç®—å¤±è´¥: {e}")

    print("=" * 60)
    return flops if THOP_AVAILABLE else (flops_fvcore if FVCORE_AVAILABLE else None)


# åœ¨æ¨¡å‹åˆå§‹åŒ–åç«‹å³è®¡ç®—FLOPs
vit_sparse.set_pretraining_mode(True)
calculate_flops(
    vit_sparse,
    input_sizes=[(1, 3, args.imagesize, args.imagesize)]
)

learner = Model(
    vit,
    vit_sparse,
    args.imagesize,
    hidden_layer='to_cls_token',
    projection_size=args.dim,
    sparse_projection_size=args.sparsedim,
    projection_hidden_size=4096
)

# åŸæœ‰çš„å‚æ•°é‡è®¡ç®— (ä¿ç•™)
total_params = sum(p.numel() for p in vit.parameters())
print(f'{total_params:,} total parameters.')
total_trainable_params = sum(
    p.numel() for p in vit.parameters() if p.requires_grad)
print(f'{total_trainable_params:,} training parameters.')

# åŸæœ‰çš„å‚æ•°é‡è®¡ç®— (ä¿ç•™)
total_params = sum(p.numel() for p in vit_sparse.parameters())
print(f'{total_params:,} total parameters.')
total_trainable_params = sum(
    p.numel() for p in vit_sparse.parameters() if p.requires_grad)
print(f'{total_trainable_params:,} training parameters.')

opt = torch.optim.Adam(learner.parameters(), lr=args.lr)
scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(opt, int(args.n_epochs / 2) + 1)
scheduler = GradualWarmupScheduler(opt, multiplier=2, total_epoch=int(args.n_epochs / 2) + 1,
                                   after_scheduler=scheduler_cosine)

data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop((args.imagesize, args.imagesize)),
        transforms.RandomVerticalFlip(),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize((args.imagesize, args.imagesize)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'test': transforms.Compose([
        transforms.Resize((args.imagesize, args.imagesize)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
}

train_dataset = torchvision.datasets.ImageFolder(root='./data_split/synthetic_train',  #synthetic_train
                                                 transform=data_transforms['train'])
trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=int(args.bs), shuffle=True, num_workers=0)

train_dataset_scd = torchvision.datasets.ImageFolder(root='./data_split/train', transform=data_transforms['train'])
trainloader_scd = torch.utils.data.DataLoader(train_dataset_scd, batch_size=int(args.bs), shuffle=True, num_workers=0,
                                              drop_last=True)

if args.cos:
    from warmup_scheduler import GradualWarmupScheduler

criterion = nn.CrossEntropyLoss().to(device)

#####################################################################

# æµ‹è¯•å•ä¸ªæ–‡ä»¶å¤¹å‡½æ•°ï¼ˆä¸resnet_3t.pyä¿æŒä¸€è‡´ï¼‰
def test_single_folder(folder_path, transform, learner, criterion, device):
    """æµ‹è¯•å•ä¸ªæ–‡ä»¶å¤¹å¹¶è¿”å›è¯„ä¼°æŒ‡æ ‡ï¼ˆäºŒåˆ†ç±»ï¼Œä¸resnet_3t.pyä¿æŒä¸€è‡´ï¼‰"""
    # åŠ è½½å•ä¸ªæµ‹è¯•é›†
    testset = torchvision.datasets.ImageFolder(root=folder_path, transform=transform)
    testloader = torch.utils.data.DataLoader(testset, batch_size=args.bs, shuffle=True, num_workers=0)

    test_loss = 0
    correct = 0
    total = 0
    all_targets = []  # æ”¶é›†æ‰€æœ‰çœŸå®æ ‡ç­¾
    all_probs = []  # æ”¶é›†æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
    all_preds = []  # æ”¶é›†æ‰€æœ‰é¢„æµ‹ç»“æœ

    learner.eval()
    start_time = time.time()

    with torch.no_grad():
        for batch_idx, (inputs, targets) in enumerate(testloader):
            inputs, targets = inputs.to(device), targets.to(device)
            _, outputs = learner.sparse_encoder.net(inputs)
            loss = criterion(outputs, targets)
            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            probs = torch.softmax(outputs, dim=1)

            all_targets.extend(targets.cpu().numpy())
            all_preds.extend(predicted.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"æ–‡ä»¶å¤¹ {folder_path} æµ‹è¯•æ—¶é—´: {elapsed_time:.6f}ç§’")

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_targets_np = np.array(all_targets)
    all_preds_np = np.array(all_preds)
    all_probs_np = np.array(all_probs)

    # è·å–ç±»åˆ«åç§°
    class_names = testset.classes

    # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼ˆä¸resnet_3t.pyä¿æŒä¸€è‡´ï¼‰
    metrics = calculate_all_metrics(all_targets_np, all_preds_np, all_probs_np, class_names)

    # è®¡ç®—å‡†ç¡®ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰
    acc = 100. * correct / total

    # è¾“å‡ºå½“å‰æ–‡ä»¶å¤¹çš„ç»“æœ
    print(f"æµ‹è¯•æ–‡ä»¶å¤¹ {folder_path}:")
    print(f"  æµ‹è¯•æŸå¤±: {test_loss:.5f}")
    print(f"  å‡†ç¡®ç‡: {acc:.5f}% ({metrics['accuracy']:.5f})")
    print(f"  æ•æ„Ÿåº¦: {metrics['sensitivity']:.5f}")
    print(f"  ç‰¹å¼‚åº¦: {metrics['specificity']:.5f}")
    print(f"  ç²¾ç¡®ç‡: {metrics['precision']:.5f}")
    print(f"  F1åˆ†æ•°: {metrics['f1_score']:.5f}")
    print(f"  ROC AUC: {metrics['roc_auc']:.5f}")
    print(f"  PR AUC: {metrics['pr_auc']:.5f}")
    print(f"  æ··æ·†çŸ©é˜µ:\n{metrics['confusion_matrix']}")

    return {
        'loss': test_loss,
        'acc': acc,  # ç™¾åˆ†æ¯”å‡†ç¡®ç‡
        'accuracy': metrics['accuracy'],  # å°æ•°å‡†ç¡®ç‡
        'sensitivity': metrics['sensitivity'],
        'specificity': metrics['specificity'],
        'precision': metrics['precision'],
        'f1_score': metrics['f1_score'],
        'roc_auc': metrics['roc_auc'],
        'pr_auc': metrics['pr_auc'],
        'class_metrics': metrics['class_metrics'],
        'confusion_matrix': metrics['confusion_matrix'],
        'fpr': metrics['fpr'],
        'tpr': metrics['tpr'],
        'all_targets': all_targets_np,
        'all_probs': all_probs_np,
        'class_names': class_names
    }


def test_all_folders(epoch, base_path, transform, learner, criterion, device, num_folders=1):
    """æµ‹è¯•æ‰€æœ‰æ–‡ä»¶å¤¹å¹¶è®¡ç®—å¹³å‡æŒ‡æ ‡ï¼ˆæ›´æ–°ç‰ˆï¼‰"""
    # å­˜å‚¨æ‰€æœ‰æ–‡ä»¶å¤¹çš„è¯„ä¼°æŒ‡æ ‡
    all_metrics = []

    # å¾ªç¯æµ‹è¯•æ–‡ä»¶å¤¹
    for i in range(num_folders):
        folder_path = f'{base_path}/test{i}/'
        print(f"\n{'=' * 60}")
        print(f"å¼€å§‹æµ‹è¯•æ–‡ä»¶å¤¹: {folder_path}")
        print(f"{'=' * 60}")
        metrics = test_single_folder(folder_path, transform, learner, criterion, device)
        all_metrics.append(metrics)

    # è®¡ç®—æ‰€æœ‰æ–‡ä»¶å¤¹çš„å¹³å‡æŒ‡æ ‡
    avg_metrics = {
        'loss': np.mean([m['loss'] for m in all_metrics]),
        'acc': np.mean([m['acc'] for m in all_metrics]),
        'accuracy': np.mean([m['accuracy'] for m in all_metrics]),
        'sensitivity': np.mean([m['sensitivity'] for m in all_metrics]),
        'specificity': np.mean([m['specificity'] for m in all_metrics]),
        'precision': np.mean([m['precision'] for m in all_metrics]),
        'f1_score': np.mean([m['f1_score'] for m in all_metrics]),
        'roc_auc': np.mean([m['roc_auc'] for m in all_metrics]),
        'pr_auc': np.mean([m['pr_auc'] for m in all_metrics])
    }

    # åˆå¹¶æ‰€æœ‰æ ·æœ¬ç”¨äºæ•´ä½“ROCè®¡ç®—
    all_targets = np.concatenate([m['all_targets'] for m in all_metrics])
    all_probs = np.concatenate([m['all_probs'] for m in all_metrics])

    # è®¡ç®—æ•´ä½“ROCæ›²çº¿
    if len(np.unique(all_targets)) >= 2:
        fpr, tpr, _ = roc_curve(all_targets, all_probs[:, 1])
        overall_roc_auc = auc(fpr, tpr)
    else:
        fpr, tpr = np.array([0, 1]), np.array([0, 1])
        overall_roc_auc = 0.5

    # è¾“å‡ºæœ€ç»ˆçš„å¹³å‡æŒ‡æ ‡
    print(f"\n{'=' * 60}")
    print("æµ‹è¯•æ–‡ä»¶å¤¹çš„å¹³å‡æŒ‡æ ‡:")
    print(f"{'=' * 60}")
    print(f"å¹³å‡æµ‹è¯•æŸå¤±: {avg_metrics['loss']:.5f}")
    print(f"å¹³å‡å‡†ç¡®ç‡: {avg_metrics['acc']:.5f}% ({avg_metrics['accuracy']:.5f})")
    print(f"å¹³å‡æ•æ„Ÿåº¦: {avg_metrics['sensitivity']:.5f}")
    print(f"å¹³å‡ç‰¹å¼‚åº¦: {avg_metrics['specificity']:.5f}")
    print(f"å¹³å‡ç²¾ç¡®ç‡: {avg_metrics['precision']:.5f}")
    print(f"å¹³å‡F1åˆ†æ•°: {avg_metrics['f1_score']:.5f}")
    print(f"å¹³å‡ROC AUC: {avg_metrics['roc_auc']:.5f}")
    print(f"å¹³å‡PR AUC: {avg_metrics['pr_auc']:.5f}")
    print(f"æ•´ä½“ROC AUC: {overall_roc_auc:.5f}")

    # è¿”å›å…³é”®æŒ‡æ ‡å’ŒROCæ›²çº¿æ•°æ®
    return {
        'avg_metrics': avg_metrics,
        'overall_fpr': fpr,
        'overall_tpr': tpr,
        'overall_roc_auc': overall_roc_auc,
        'all_metrics': all_metrics
    }


def val_all_folders(epoch, base_path, transform, learner, criterion, device, num_folders=1):
    """éªŒè¯æ‰€æœ‰æ–‡ä»¶å¤¹å¹¶è®¡ç®—å¹³å‡æŒ‡æ ‡ï¼ˆæ›´æ–°ç‰ˆï¼‰"""
    # å­˜å‚¨æ‰€æœ‰æ–‡ä»¶å¤¹çš„è¯„ä¼°æŒ‡æ ‡
    all_metrics = []

    # å¾ªç¯éªŒè¯æ–‡ä»¶å¤¹
    for i in range(num_folders):
        folder_path = f'{base_path}/val{i}/'
        print(f"\n{'=' * 60}")
        print(f"å¼€å§‹éªŒè¯æ–‡ä»¶å¤¹: {folder_path}")
        print(f"{'=' * 60}")
        metrics = test_single_folder(folder_path, transform, learner, criterion, device)
        all_metrics.append(metrics)

    # è®¡ç®—æ‰€æœ‰æ–‡ä»¶å¤¹çš„å¹³å‡æŒ‡æ ‡
    avg_metrics = {
        'loss': np.mean([m['loss'] for m in all_metrics]),
        'acc': np.mean([m['acc'] for m in all_metrics]),
        'accuracy': np.mean([m['accuracy'] for m in all_metrics]),
        'sensitivity': np.mean([m['sensitivity'] for m in all_metrics]),
        'specificity': np.mean([m['specificity'] for m in all_metrics]),
        'precision': np.mean([m['precision'] for m in all_metrics]),
        'f1_score': np.mean([m['f1_score'] for m in all_metrics]),
        'roc_auc': np.mean([m['roc_auc'] for m in all_metrics]),
        'pr_auc': np.mean([m['pr_auc'] for m in all_metrics])
    }

    print(f"\n{'=' * 60}")
    print("éªŒè¯æ–‡ä»¶å¤¹çš„å¹³å‡æŒ‡æ ‡:")
    print(f"{'=' * 60}")
    print(f"å¹³å‡éªŒè¯æŸå¤±: {avg_metrics['loss']:.5f}")
    print(f"å¹³å‡å‡†ç¡®ç‡: {avg_metrics['acc']:.5f}% ({avg_metrics['accuracy']:.5f})")
    print(f"å¹³å‡æ•æ„Ÿåº¦: {avg_metrics['sensitivity']:.5f}")
    print(f"å¹³å‡ç‰¹å¼‚åº¦: {avg_metrics['specificity']:.5f}")
    print(f"å¹³å‡ç²¾ç¡®ç‡: {avg_metrics['precision']:.5f}")
    print(f"å¹³å‡F1åˆ†æ•°: {avg_metrics['f1_score']:.5f}")
    print(f"å¹³å‡ROC AUC: {avg_metrics['roc_auc']:.5f}")
    print(f"å¹³å‡PR AUC: {avg_metrics['pr_auc']:.5f}")

    # è¿”å›å¹³å‡æ•æ„Ÿåº¦å’Œå¹³å‡AUCä½œä¸ºéªŒè¯æŒ‡æ ‡
    return avg_metrics['sensitivity'], avg_metrics['specificity'], avg_metrics['roc_auc']


learner.online_encoder.net.set_pretraining_mode(False)

# ç¬¬ä¸€é˜¶æ®µè®­ç»ƒ
best_sen = 0  # æœ€ä½³æ•æ„Ÿåº¦
best_spec = 0  # æœ€ä½³ç‰¹å¼‚åº¦
accumulation = 4
best_acc_global = 0  # best test accuracy
best_roc_global = 0  # best test roc
biaozhi = 0
total_batch = 0

# å­˜å‚¨ç¬¬ä¸€é˜¶æ®µè®­ç»ƒå†å²
first_stage_train_history = {
    'train_losses': [],
    'val_losses': [],
    'train_accs': [],
    'val_accs': [],
    'val_sensitivities': [],
    'val_specificities': [],
    'best_sensitivity': 0.0,
    'best_specificity': 0.0,
    'best_acc': 0.0,
    'best_epoch': 0
}

for interation in range(args.n_epochs):  # args.n_epochs
    print('interation=%d' % interation)
    torch.cuda.synchronize()
    start = time.time()
    train_loss = 0
    learner.train()

    for batch_idx, (inputs, targets) in enumerate(trainloader):
        inputs, targets = inputs.to(device), targets.to(device)

        img, label = inputs.to(device), targets.to(device)

        loss = learner(img, label)

        train_loss += loss.item()
        loss.backward()

        if ((batch_idx + 1) % accumulation) == 0:
            opt.step()
            opt.zero_grad()
        total_batch = batch_idx

    train_loss = train_loss / (total_batch + 1)
    first_stage_train_history['train_losses'].append(train_loss)

    content = time.ctime() + ' ' + f'Epoch {interation}, Train loss: {train_loss:.4f}, lr: {opt.param_groups[0]["lr"]:.5f}'
    print(content)

    if interation >= args.n_epochs-3:
        base_path = './data_split'
        val_sen, val_spec, val_roc = val_all_folders(interation, base_path, data_transforms['val'], learner, criterion, device)

        # æ›´æ–°éªŒè¯æŒ‡æ ‡
        first_stage_train_history['val_losses'].append(train_loss)  # ç®€åŒ–ï¼Œå®é™…åº”è¯¥è®¡ç®—éªŒè¯æŸå¤±
        first_stage_train_history['val_accs'].append(val_roc)  # ä½¿ç”¨AUCä½œä¸ºå‡†ç¡®ç‡ä»£ç†
        first_stage_train_history['val_sensitivities'].append(val_sen)
        first_stage_train_history['val_specificities'].append(val_spec)

        # æ ¹æ®æ•æ„Ÿåº¦é˜ˆå€¼å†³å®šä¿å­˜æ¨¡å‹çš„ç­–ç•¥
        save_model = False
        if val_sen < 0.95:
            # æ•æ„Ÿåº¦å°äº0.95æ—¶ï¼Œä¿å­˜æ•æ„Ÿåº¦æœ€å¤§çš„æ¨¡å‹
            if val_sen > best_sen:
                best_sen = val_sen
                first_stage_train_history['best_sensitivity'] = val_sen
                first_stage_train_history['best_specificity'] = val_spec
                first_stage_train_history['best_acc'] = val_roc
                first_stage_train_history['best_epoch'] = interation + 1
                torch.save(learner.state_dict(), './improved-net.pth')
                save_model = True
                print(f"ğŸš€ ç¬¬ä¸€é˜¶æ®µæ–°çš„æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºæ•æ„Ÿåº¦ï¼‰! éªŒè¯æ•æ„Ÿåº¦: {best_sen:.4f}, ç‰¹å¼‚åº¦: {val_spec:.4f}, AUC: {val_roc:.4f}")
        else:
            # æ•æ„Ÿåº¦å¤§äºç­‰äº0.95æ—¶ï¼Œä¿å­˜ç‰¹å¼‚åº¦æœ€å¤§çš„æ¨¡å‹
            if val_spec > best_spec:
                best_spec = val_spec
                first_stage_train_history['best_sensitivity'] = val_sen
                first_stage_train_history['best_specificity'] = val_spec
                first_stage_train_history['best_acc'] = val_roc
                first_stage_train_history['best_epoch'] = interation + 1
                torch.save(learner.state_dict(), './improved-net.pth')
                save_model = True
                print(f"ğŸš€ ç¬¬ä¸€é˜¶æ®µæ–°çš„æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºç‰¹å¼‚åº¦ï¼‰! éªŒè¯æ•æ„Ÿåº¦: {val_sen:.4f}, ç‰¹å¼‚åº¦: {best_spec:.4f}, AUC: {val_roc:.4f}")

    scheduler.step(interation)
    torch.cuda.synchronize()


# è®¡ç®—ç¬¬äºŒé˜¶æ®µè®­ç»ƒé›†çš„ç±»åˆ«æƒé‡
def calculate_class_weights(dataset, num_classes):
    """è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡"""
    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
    class_counts = torch.zeros(num_classes)
    for _, label in dataset:
        class_counts[label] += 1

    # ä½¿ç”¨é€†é¢‘ç‡è®¡ç®—æƒé‡
    total_samples = len(dataset)
    weights = total_samples / (num_classes * class_counts)

    # ç¡®ä¿æ²¡æœ‰æ— é™å¤§çš„æƒé‡
    weights = torch.clamp(weights, min=0.1, max=10.0)

    return weights.tolist()

# ç¬¬äºŒé˜¶æ®µï¼šä¸‰æ¬¡ç‹¬ç«‹è®­ç»ƒéªŒè¯æµ‹è¯•
print("\n" + "=" * 80)
print("å¼€å§‹ç¬¬äºŒé˜¶æ®µï¼šä¸‰æ¬¡ç‹¬ç«‹è®­ç»ƒéªŒè¯æµ‹è¯•")
print("=" * 80)

# å­˜å‚¨æ‰€æœ‰å®éªŒç»“æœçš„åˆ—è¡¨
all_experiment_results = []
all_train_histories = []
all_test_results = []  # å­˜å‚¨æ¯æ¬¡å®éªŒçš„è¯¦ç»†æµ‹è¯•ç»“æœ
all_roc_data = []
all_auc_values = []

# åˆ›å»ºç»“æœç›®å½•
os.makedirs('./results', exist_ok=True)

# è¿›è¡Œä¸‰æ¬¡ç‹¬ç«‹å®éªŒ
num_experiments = 3
for exp_id in range(num_experiments):
    print(f"\n{'=' * 60}")
    print(f"ç¬¬äºŒé˜¶æ®µå®éªŒ #{exp_id + 1}/{num_experiments}")
    print(f"{'=' * 60}")

    # è®¾ç½®ä¸åŒçš„éšæœºç§å­ä»¥ç¡®ä¿ç‹¬ç«‹æ€§
    set_seed(seed=42 + exp_id * 100)

    # é‡æ–°åŠ è½½ç¬¬ä¸€é˜¶æ®µè®­ç»ƒåçš„æ¨¡å‹
    state_dict = torch.load('./improved-net.pth')
    missing_keys, unexpected_keys = learner.load_state_dict(state_dict, strict=False)
    if missing_keys:
        print(f"è­¦å‘Š: åŠ è½½çŠ¶æ€å­—å…¸æ—¶ç¼ºå¤±ä»¥ä¸‹é”®: {missing_keys}")
    if unexpected_keys:
        print(f"è­¦å‘Š: åŠ è½½çŠ¶æ€å­—å…¸æ—¶å‡ºç°æ„å¤–é”®: {unexpected_keys}")

    # è®¡ç®—å¹¶è®¾ç½®ç±»åˆ«æƒé‡ï¼ˆé’ˆå¯¹éå‡è¡¡æ ·æœ¬è®­ç»ƒé˜¶æ®µï¼‰
    class_weights = calculate_class_weights(train_dataset_scd, args.num_classes)
    print(f"å®éªŒ #{exp_id + 1} ç±»åˆ«æƒé‡: {class_weights}")
    learner.set_class_weights(class_weights)

    # é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    opt_scd = torch.optim.Adam(learner.parameters(), lr=args.lr * 0.1)
    scheduler_cosine_scd = torch.optim.lr_scheduler.CosineAnnealingLR(opt_scd, int(args.n_epochs / 2) + 1)
    scheduler_scd = GradualWarmupScheduler(opt_scd, multiplier=2, total_epoch=int(args.n_epochs / 2) + 1,
                                           after_scheduler=scheduler_cosine_scd)

    # è®­ç»ƒå¾ªç¯
    best_sen = 0  # æœ€ä½³æ•æ„Ÿåº¦
    best_spec = 0  # æœ€ä½³ç‰¹å¼‚åº¦
    best_roc = 0
    best_model_path = f'./results/stage2_best_model_exp{exp_id + 1}.pth'

    # å­˜å‚¨ç¬¬äºŒé˜¶æ®µè®­ç»ƒå†å²
    stage2_train_history = {
        'train_losses': [],
        'val_losses': [],
        'train_accs': [],
        'val_accs': [],
        'val_sensitivities': [],
        'val_specificities': [],
        'best_sensitivity': 0.0,
        'best_specificity': 0.0,
        'best_acc': 0.0,
        'best_epoch': 0
    }

    for interation in range(args.n_epochs):
        print('interation=%d' % interation)
        torch.cuda.synchronize()
        train_loss = 0
        learner.train()

        for batch_idx, (inputs, targets) in enumerate(trainloader_scd):
            inputs, targets = inputs.to(device), targets.to(device)

            img, label = inputs.to(device), targets.to(device)

            loss = learner(img, label)

            train_loss += loss.item()
            loss.backward()

            if ((batch_idx + 1) % accumulation) == 0:
                opt_scd.step()
                opt_scd.zero_grad()

        train_loss = train_loss / (len(trainloader_scd) + 1e-5)
        stage2_train_history['train_losses'].append(train_loss)

        content = time.ctime() + ' ' + f'Epoch {interation}, Train loss: {train_loss:.4f}, lr: {opt_scd.param_groups[0]["lr"]:.5f}'
        print(content)

        # éªŒè¯
        if interation >= args.n_epochs - 5:
            base_path = './data_split'
            val_sen, val_spec, val_roc = val_all_folders(interation, base_path, data_transforms['test'], learner, criterion, device)

            # æ›´æ–°éªŒè¯æŒ‡æ ‡
            stage2_train_history['val_losses'].append(train_loss)  # ç®€åŒ–
            stage2_train_history['val_accs'].append(val_roc)
            stage2_train_history['val_sensitivities'].append(val_sen)
            stage2_train_history['val_specificities'].append(val_spec)

            # æ ¹æ®æ•æ„Ÿåº¦é˜ˆå€¼å†³å®šä¿å­˜æ¨¡å‹çš„ç­–ç•¥
            save_model = False
            if val_sen < 0.95:
                # æ•æ„Ÿåº¦å°äº0.95æ—¶ï¼Œä¿å­˜æ•æ„Ÿåº¦æœ€å¤§çš„æ¨¡å‹
                if val_sen > best_sen:
                    best_sen = val_sen
                    best_roc = val_roc
                    stage2_train_history['best_sensitivity'] = val_sen
                    stage2_train_history['best_specificity'] = val_spec
                    stage2_train_history['best_acc'] = val_roc
                    stage2_train_history['best_epoch'] = interation + 1
                    torch.save(learner.state_dict(), best_model_path)
                    save_model = True
                    print(f"ğŸš€ å®éªŒ #{exp_id + 1} æ–°çš„æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºæ•æ„Ÿåº¦ï¼‰! éªŒè¯æ•æ„Ÿåº¦: {best_sen:.4f}, ç‰¹å¼‚åº¦: {val_spec:.4f}, AUC: {val_roc:.4f}")
            else:
                # æ•æ„Ÿåº¦å¤§äºç­‰äº0.95æ—¶ï¼Œä¿å­˜ç‰¹å¼‚åº¦æœ€å¤§çš„æ¨¡å‹
                if val_spec > best_spec:
                    best_sen = val_sen
                    best_spec = val_spec
                    best_roc = val_roc
                    stage2_train_history['best_sensitivity'] = val_sen
                    stage2_train_history['best_specificity'] = val_spec
                    stage2_train_history['best_acc'] = val_roc
                    stage2_train_history['best_epoch'] = interation + 1
                    torch.save(learner.state_dict(), best_model_path)
                    save_model = True
                    print(f"ğŸš€ å®éªŒ #{exp_id + 1} æ–°çš„æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºç‰¹å¼‚åº¦ï¼‰! éªŒè¯æ•æ„Ÿåº¦: {val_sen:.4f}, ç‰¹å¼‚åº¦: {best_spec:.4f}, AUC: {val_roc:.4f}")

        scheduler_scd.step(interation)

    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•
    learner.load_state_dict(torch.load(best_model_path))

    # åœ¨æµ‹è¯•é›†ä¸Šæµ‹è¯•æœ€ä½³æ¨¡å‹
    print(f"\nå®éªŒ #{exp_id + 1} åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
    base_path = './data_split'
    test_result = test_all_folders(
        interation, base_path, data_transforms['test'], learner, criterion, device
    )

    # å­˜å‚¨ROCæ›²çº¿æ•°æ®
    all_roc_data.append((test_result['overall_fpr'], test_result['overall_tpr']))
    all_auc_values.append(test_result['overall_roc_auc'])

    # æ”¶é›†å®éªŒæ•°æ®
    experiment_data = {
        'experiment_id': exp_id + 1,
        'best_sensitivity': stage2_train_history['best_sensitivity'],
        'best_specificity': stage2_train_history['best_specificity'],
        'best_roc_auc': best_roc,
        'test_metrics': test_result['avg_metrics'],
        'overall_roc_auc': test_result['overall_roc_auc'],
        'train_history': stage2_train_history
    }

    all_experiment_results.append(experiment_data)
    all_train_histories.append(stage2_train_history)
    all_test_results.append(test_result)

    # æ‰“å°æœ¬æ¬¡å®éªŒçš„ç»“æœ
    print(f"\nå®éªŒ #{exp_id + 1} ç»“æœ:")
    print(f"- æœ€ä½³éªŒè¯æ•æ„Ÿåº¦: {stage2_train_history['best_sensitivity']:.4f}")
    print(f"- æœ€ä½³éªŒè¯ç‰¹å¼‚åº¦: {stage2_train_history['best_specificity']:.4f}")
    print(f"- æœ€ä½³éªŒè¯AUC: {best_roc:.4f}")
    print(f"- æµ‹è¯•é›†å‡†ç¡®ç‡: {test_result['avg_metrics']['accuracy']:.4f}")
    print(f"- æµ‹è¯•é›†æ•æ„Ÿåº¦: {test_result['avg_metrics']['sensitivity']:.4f}")
    print(f"- æµ‹è¯•é›†ç‰¹å¼‚åº¦: {test_result['avg_metrics']['specificity']:.4f}")
    print(f"- æµ‹è¯•é›†ç²¾ç¡®ç‡: {test_result['avg_metrics']['precision']:.4f}")
    print(f"- æµ‹è¯•é›†F1åˆ†æ•°: {test_result['avg_metrics']['f1_score']:.4f}")
    print(f"- æµ‹è¯•é›†ROC AUC: {test_result['avg_metrics']['roc_auc']:.4f}")
    print(f"- æ•´ä½“ROC AUC: {test_result['overall_roc_auc']:.4f}")

# ç»˜åˆ¶å¹³å‡ROCæ›²çº¿Â±æ ‡å‡†å·®åŒºé—´
print(f"\n{'=' * 80}")
print("ç»˜åˆ¶å¹³å‡ROCæ›²çº¿å’Œæ ‡å‡†å·®åŒºé—´")
print(f"{'=' * 80}")

if all_roc_data and len(all_roc_data) == num_experiments:
    mean_auc, std_auc = plot_mean_roc_curve_with_std(
        all_roc_data, all_auc_values,
        model_name="MobileViT_Stage2",
        save_path='./results'
    )
else:
    print("è­¦å‘Š: æ— æ³•è®¡ç®—å¹³å‡ROCæ›²çº¿ï¼ŒROCæ•°æ®ä¸å®Œæ•´")

# ç”Ÿæˆç»¼åˆæŠ¥å‘Šï¼ˆä¸resnet_3t.pyç±»ä¼¼ï¼‰
print(f"\n{'=' * 80}")
print("ä¸‰æ¬¡ç‹¬ç«‹å®éªŒç»¼åˆæŠ¥å‘Š")
print(f"{'=' * 80}")

# è®¡ç®—å„é¡¹æŒ‡æ ‡çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®
metrics_to_analyze = ['accuracy', 'sensitivity', 'specificity', 'precision', 'f1_score', 'roc_auc']

# æå–æ¯æ¬¡å®éªŒçš„æµ‹è¯•æŒ‡æ ‡
all_test_metrics = []
for exp_result in all_experiment_results:
    test_metrics = exp_result['test_metrics']
    all_test_metrics.append(test_metrics)

metrics_summary = {}
for metric in metrics_to_analyze:
    values = [test_metrics[metric] for test_metrics in all_test_metrics]
    metrics_summary[metric] = {
        'mean': np.mean(values),
        'std': np.std(values),
        'min': np.min(values),
        'max': np.max(values),
        'values': values
    }

# è®¡ç®—æœ€ä½³éªŒè¯æŒ‡æ ‡çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®
best_sensitivities = [exp['best_sensitivity'] for exp in all_experiment_results]
best_specificities = [exp['best_specificity'] for exp in all_experiment_results]
best_roc_aucs = [exp['best_roc_auc'] for exp in all_experiment_results]

best_metrics_summary = {
    'best_sensitivity': {
        'mean': np.mean(best_sensitivities),
        'std': np.std(best_sensitivities),
        'min': np.min(best_sensitivities),
        'max': np.max(best_sensitivities),
        'values': best_sensitivities
    },
    'best_specificity': {
        'mean': np.mean(best_specificities),
        'std': np.std(best_specificities),
        'min': np.min(best_specificities),
        'max': np.max(best_specificities),
        'values': best_specificities
    },
    'best_roc_auc': {
        'mean': np.mean(best_roc_aucs),
        'std': np.std(best_roc_aucs),
        'min': np.min(best_roc_aucs),
        'max': np.max(best_roc_aucs),
        'values': best_roc_aucs
    }
}

# æ‰“å°æŠ¥å‘Š
print(f"\næ¨¡å‹: MobileViT (ç¬¬äºŒé˜¶æ®µ)")
print(f"å®éªŒæ¬¡æ•°: {num_experiments}")
print(f"è®­ç»ƒè½®æ•°: {args.n_epochs}")

print(f"\næœ€ä½³éªŒè¯æ€§èƒ½ç»Ÿè®¡ (å‡å€¼ Â± æ ‡å‡†å·®):")
print(f"  æœ€ä½³éªŒè¯æ•æ„Ÿåº¦: {best_metrics_summary['best_sensitivity']['mean']:.4f} Â± {best_metrics_summary['best_sensitivity']['std']:.4f}")
print(f"  æœ€ä½³éªŒè¯ç‰¹å¼‚åº¦: {best_metrics_summary['best_specificity']['mean']:.4f} Â± {best_metrics_summary['best_specificity']['std']:.4f}")
print(f"  æœ€ä½³éªŒè¯AUC: {best_metrics_summary['best_roc_auc']['mean']:.4f} Â± {best_metrics_summary['best_roc_auc']['std']:.4f}")

print(f"\næµ‹è¯•é›†æ€§èƒ½ç»Ÿè®¡ (å‡å€¼ Â± æ ‡å‡†å·®):")
metric_names = {
    'accuracy': 'å‡†ç¡®ç‡',
    'sensitivity': 'æ•æ„Ÿåº¦',
    'specificity': 'ç‰¹å¼‚åº¦',
    'precision': 'ç²¾ç¡®ç‡',
    'f1_score': 'F1åˆ†æ•°',
    'roc_auc': 'ROC AUC'
}

for metric, stats in metrics_summary.items():
    print(f"  {metric_names.get(metric, metric)}: "
          f"{stats['mean']:.4f} Â± {stats['std']:.4f} "
          f"(èŒƒå›´: {stats['min']:.4f} - {stats['max']:.4f})")

# ä¿å­˜æ‰€æœ‰å®éªŒç»“æœåˆ°CSVæ–‡ä»¶
print(f"\n{'=' * 80}")
print("ä¿å­˜æ‰€æœ‰å®éªŒç»“æœ")
print(f"{'=' * 80}")

# åˆ›å»ºç»“æœDataFrame
results_df = pd.DataFrame({
    'å®éªŒç¼–å·': [f'å®éªŒ{i + 1}' for i in range(num_experiments)],
    'æœ€ä½³éªŒè¯æ•æ„Ÿåº¦': [exp['best_sensitivity'] for exp in all_experiment_results],
    'æœ€ä½³éªŒè¯ç‰¹å¼‚åº¦': [exp['best_specificity'] for exp in all_experiment_results],
    'æœ€ä½³éªŒè¯AUC': [exp['best_roc_auc'] for exp in all_experiment_results],
    'æµ‹è¯•å‡†ç¡®ç‡': [metrics_summary['accuracy']['values'][i] for i in range(num_experiments)],
    'æµ‹è¯•æ•æ„Ÿåº¦': [metrics_summary['sensitivity']['values'][i] for i in range(num_experiments)],
    'æµ‹è¯•ç‰¹å¼‚åº¦': [metrics_summary['specificity']['values'][i] for i in range(num_experiments)],
    'æµ‹è¯•ç²¾ç¡®ç‡': [metrics_summary['precision']['values'][i] for i in range(num_experiments)],
    'æµ‹è¯•F1åˆ†æ•°': [metrics_summary['f1_score']['values'][i] for i in range(num_experiments)],
    'æµ‹è¯•ROC_AUC': [metrics_summary['roc_auc']['values'][i] for i in range(num_experiments)]
})

# æ·»åŠ ç»Ÿè®¡è¡Œ
stats_row = {
    'å®éªŒç¼–å·': 'ç»Ÿè®¡é‡',
    'æœ€ä½³éªŒè¯æ•æ„Ÿåº¦': f"{best_metrics_summary['best_sensitivity']['mean']:.4f} Â± {best_metrics_summary['best_sensitivity']['std']:.4f}",
    'æœ€ä½³éªŒè¯ç‰¹å¼‚åº¦': f"{best_metrics_summary['best_specificity']['mean']:.4f} Â± {best_metrics_summary['best_specificity']['std']:.4f}",
    'æœ€ä½³éªŒè¯AUC': f"{best_metrics_summary['best_roc_auc']['mean']:.4f} Â± {best_metrics_summary['best_roc_auc']['std']:.4f}",
    'æµ‹è¯•å‡†ç¡®ç‡': f"{metrics_summary['accuracy']['mean']:.4f} Â± {metrics_summary['accuracy']['std']:.4f}",
    'æµ‹è¯•æ•æ„Ÿåº¦': f"{metrics_summary['sensitivity']['mean']:.4f} Â± {metrics_summary['sensitivity']['std']:.4f}",
    'æµ‹è¯•ç‰¹å¼‚åº¦': f"{metrics_summary['specificity']['mean']:.4f} Â± {metrics_summary['specificity']['std']:.4f}",
    'æµ‹è¯•ç²¾ç¡®ç‡': f"{metrics_summary['precision']['mean']:.4f} Â± {metrics_summary['precision']['std']:.4f}",
    'æµ‹è¯•F1åˆ†æ•°': f"{metrics_summary['f1_score']['mean']:.4f} Â± {metrics_summary['f1_score']['std']:.4f}",
    'æµ‹è¯•ROC_AUC': f"{metrics_summary['roc_auc']['mean']:.4f} Â± {metrics_summary['roc_auc']['std']:.4f}"
}

results_df = pd.concat([results_df, pd.DataFrame([stats_row])], ignore_index=True)

# ä¿å­˜åˆ°CSV
results_csv_path = './results/stage2_three_experiments_summary.csv'
results_df.to_csv(results_csv_path, index=False, encoding='utf-8-sig')
print(f"å®éªŒç»“æœå·²ä¿å­˜åˆ°: {results_csv_path}")

# ä¿å­˜è¯¦ç»†çš„æµ‹è¯•ç»“æœ
detailed_results = []
for i, test_result in enumerate(all_test_results):
    for j, folder_metrics in enumerate(test_result['all_metrics']):
        detailed_results.append({
            'å®éªŒç¼–å·': f'å®éªŒ{i + 1}',
            'æ–‡ä»¶å¤¹': f'test{j}',
            'å‡†ç¡®ç‡': folder_metrics['accuracy'],
            'æ•æ„Ÿåº¦': folder_metrics['sensitivity'],
            'ç‰¹å¼‚åº¦': folder_metrics['specificity'],
            'ç²¾ç¡®ç‡': folder_metrics['precision'],
            'F1åˆ†æ•°': folder_metrics['f1_score'],
            'ROC_AUC': folder_metrics['roc_auc'],
            'PR_AUC': folder_metrics['pr_auc']
        })

detailed_df = pd.DataFrame(detailed_results)
detailed_csv_path = './results/stage2_detailed_results.csv'
detailed_df.to_csv(detailed_csv_path, index=False, encoding='utf-8-sig')
print(f"è¯¦ç»†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {detailed_csv_path}")

# ä¿å­˜å®Œæ•´çš„å®éªŒæ£€æŸ¥ç‚¹
final_checkpoint = {
    'all_experiment_results': all_experiment_results,
    'all_test_metrics': all_test_metrics,
    'all_roc_data': all_roc_data,
    'all_auc_values': all_auc_values,
    'metrics_summary': metrics_summary,
    'best_metrics_summary': best_metrics_summary,
    'num_experiments': num_experiments,
    'num_epochs': args.n_epochs
}

checkpoint_path = './results/stage2_final_experiment_checkpoint.pth'
torch.save(final_checkpoint, checkpoint_path)
print(f"å®Œæ•´å®éªŒæ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {checkpoint_path}")

print(f"\n{'=' * 80}")
print("ç¬¬äºŒé˜¶æ®µä¸‰æ¬¡ç‹¬ç«‹å®éªŒå®Œæˆ!")
print(f"{'=' * 80}")