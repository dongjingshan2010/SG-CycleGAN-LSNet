#!/usr/bin/python3

import argparse
import itertools
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
import torch.utils.data.distributed
import torchvision.transforms as transforms
from torch.autograd import Variable
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
from PIL import Image
import yaml
from rectified_flow import RectifiedFlow
from unet_model import MiniUnet
from models_conv_dann import Generator, SharedGenerator
from models_conv_dann import Discriminator, Domain_Discriminator
from utils import ReplayBuffer, LambdaLR, Logger, weights_init_normal
from datasets import ImageDataset
import torch
from unet_model import MiniUnet
from rectified_flow import RectifiedFlow
import cv2
import os, math
import numpy as np
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image

os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2"

parser = argparse.ArgumentParser()
parser.add_argument('--epoch', type=int, default=30, help='starting epoch')
parser.add_argument('--n_epochs', type=int, default=30, help='number of epochs of training')
parser.add_argument('--batchSize', type=int, default=6, help='size of the batches')
parser.add_argument('--depth4vit', type=int, default=3, help='size of the batches')
parser.add_argument('--dataroot', type=str, default='./datasets/Med_shallowdeep/', help='root directory of the dataset')
parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate')
parser.add_argument('--decay_epoch', type=int, default=5,
                    help='epoch to start linearly decaying the learning rate to 0')
parser.add_argument('--mtwg_fea', type=int, default=32, help='size of the batches')
parser.add_argument('--size', type=int, default=256, help='size of the data crop (squared assumed)')
parser.add_argument('--input_nc', type=int, default=3, help='number of channels of input data')
parser.add_argument('--output_nc', type=int, default=3, help='number of channels of output data')
parser.add_argument('--cuda', default=True, action='store_true', help='use GPU computation')
parser.add_argument('--n_cpu', type=int, default=0, help='number of cpu threads to use during batch generation')
parser.add_argument('--world_size', type=int, default=1, help='number of nodes for distributed training')
parser.add_argument('--rank', type=int, default=0, help='node rank for distributed training')
parser.add_argument('--dist_url', default='tcp://127.0.0.1:12345', help='url used to set up distributed training')
parser.add_argument('--dist_backend', default='nccl', help='distributed backend')
parser.add_argument('--gpu', default=None, type=int, help='GPU id to use')
opt = parser.parse_args()

if torch.cuda.is_available() and not opt.cuda:
    print("WARNING: You have a CUDA device, so you should probably run with --cuda")


def setup(rank, world_size):
    # Initialize distributed training
    dist.init_process_group(
        backend=opt.dist_backend,
        init_method=opt.dist_url,
        world_size=world_size,
        rank=rank
    )
    torch.cuda.set_device(rank)


def cleanup():
    dist.destroy_process_group()


class GradientReversalLayer(torch.autograd.Function):
    """æ”¹è¿›çš„æ¢¯åº¦åè½¬å±‚"""

    @staticmethod
    def forward(ctx, x, alpha=1.0):
        ctx.save_for_backward(torch.tensor(alpha))
        return x

    @staticmethod
    def backward(ctx, grad_output):
        alpha, = ctx.saved_tensors
        return grad_output.neg() * alpha, None  # æ˜ç¡®ä½¿ç”¨neg()æ–¹æ³•


def main_worker(rank, world_size):
    # Initialize distributed training
    if world_size > 1:
        setup(rank, world_size)

    # Set device
    device = torch.device(f"cuda:{rank}" if torch.cuda.is_available() and opt.cuda else "cpu")
    torch.cuda.set_device(device)

    # Set global batch size
    batch_size = opt.batchSize // world_size

    # è®¾ç½®æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
    accumulation_steps = 4

    # Networks
    shared_generator = SharedGenerator(opt.input_nc, opt.output_nc, opt.mtwg_fea, opt.size, opt.depth4vit).to(device)

    # checkpoint_path = 'output/shared_generator.pth'
    # checkpoint = torch.load(checkpoint_path, map_location=device)
    #
    # # å¤„ç†DDPåŒ…è£…çš„æ¨¡å‹æƒé‡ï¼ˆå¦‚æœcheckpointæ˜¯DDPä¿å­˜çš„ï¼‰
    # state_dict = checkpoint.get('shared_generator', checkpoint)  # å…¼å®¹ä¸åŒä¿å­˜æ–¹å¼
    # if all(key.startswith('module.') for key in state_dict.keys()):
    #     # ç§»é™¤DDPæ·»åŠ çš„'module.'å‰ç¼€
    #     state_dict = {k[7:]: v for k, v in state_dict.items()}
    # # åŠ è½½æƒé‡åˆ°æ¨¡å‹
    # shared_generator.load_state_dict(state_dict, strict=True)  # strict=Trueç¡®ä¿æƒé‡å®Œå…¨åŒ¹é…
    # print(f"æˆåŠŸä» {checkpoint_path} åŠ è½½SharedGeneratorå‚æ•°")

    def get_generator():
        """æ ¹æ®æ˜¯å¦ä½¿ç”¨DDPè¿”å›æ­£ç¡®çš„ç”Ÿæˆå™¨å®ä¾‹"""
        return shared_generator.module if world_size > 1 else shared_generator

    def netG_A2B(x):
        return get_generator().forward_a2b(x)

    def netG_B2A(x):
        return get_generator().forward_b2a(x)

    # åˆ†åˆ«å®šä¹‰ä¼˜åŒ–å™¨
    optimizer_G = optim.Adam(shared_generator.parameters(), lr=opt.lr, betas=(0.5, 0.999))

    netD_A = Discriminator(opt.input_nc).to(device)
    netD_B = Discriminator(opt.output_nc).to(device)
    domain_cls = Domain_Discriminator(opt.mtwg_fea).to(device)

    # Apply weight initialization
    shared_generator.apply(weights_init_normal)
    netD_A.apply(weights_init_normal)
    netD_B.apply(weights_init_normal)
    domain_cls.apply(weights_init_normal)

    # Wrap models for distributed training
    if world_size > 1:
        shared_generator = DDP(shared_generator, device_ids=[rank])
        netD_A = DDP(netD_A, device_ids=[rank])
        netD_B = DDP(netD_B, device_ids=[rank])
        domain_cls = DDP(domain_cls, device_ids=[rank])

    # Losses
    criterion_GAN = nn.MSELoss()
    criterion_cycle = nn.L1Loss()
    criterion_identity = nn.L1Loss()
    criterion_domain = nn.CrossEntropyLoss()

    # Optimizers & LR schedulers
    optimizer_D_A = optim.Adam(netD_A.parameters(), lr=opt.lr, betas=(0.5, 0.999))
    optimizer_D_B = optim.Adam(netD_B.parameters(), lr=opt.lr, betas=(0.5, 0.999))
    optimizer_domain = optim.Adam(domain_cls.parameters(), lr=opt.lr, betas=(0.5, 0.999))

    lr_scheduler_G = optim.lr_scheduler.LambdaLR(optimizer_G,
                                                 lr_lambda=LambdaLR(opt.epoch, 0, opt.decay_epoch).step)
    lr_scheduler_D_A = optim.lr_scheduler.LambdaLR(optimizer_D_A,
                                                   lr_lambda=LambdaLR(opt.epoch, 0, opt.decay_epoch).step)
    lr_scheduler_D_B = optim.lr_scheduler.LambdaLR(optimizer_D_B,
                                                   lr_lambda=LambdaLR(opt.epoch, 0, opt.decay_epoch).step)
    lr_scheduler_domain = optim.lr_scheduler.LambdaLR(optimizer_domain,
                                                      lr_lambda=LambdaLR(opt.epoch, 0, opt.decay_epoch).step)

    # Inputs & targets memory allocation
    Tensor = torch.cuda.FloatTensor if opt.cuda else torch.Tensor
    input_A = Tensor(batch_size, opt.input_nc, opt.size, opt.size)
    input_B = Tensor(batch_size, opt.output_nc, opt.size, opt.size)
    target_real = Variable(Tensor(batch_size).fill_(1.0), requires_grad=False)
    target_fake = Variable(Tensor(batch_size).fill_(0.0), requires_grad=False)

    fake_A_buffer = ReplayBuffer()
    fake_B_buffer = ReplayBuffer()

    # Dataset loader
    transforms_ = [
        transforms.Resize(int(opt.size * 1.12), Image.BICUBIC),
        transforms.RandomCrop(opt.size),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ]

    dataset = ImageDataset(opt.dataroot, transforms_=transforms_, unaligned=True)

    # Distributed sampler if using multiple GPUs
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank) if world_size > 1 else None

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=(sampler is None),
        num_workers=opt.n_cpu,
        drop_last=True,
        sampler=sampler
    )

    # Train CycleGAN
    for epoch in range(opt.epoch, opt.epoch):
        print('Epoch {}/{}'.format(epoch, opt.epoch))
        if world_size > 1:
            sampler.set_epoch(epoch)

        for i, batch in enumerate(dataloader):
            # Set model input
            real_A = Variable(input_A.copy_(batch['A'])).to(device)
            real_B = Variable(input_B.copy_(batch['B'])).to(device)

            ###### Generators A2B and B2A ######
            # åªåœ¨ç´¯ç§¯æ­¥éª¤çš„å¼€å§‹æ—¶æ¸…é›¶æ¢¯åº¦
            if i % accumulation_steps == 0:
                optimizer_G.zero_grad()

            # Identity loss
            same_B, modefreeB = netG_A2B(real_B)
            loss_identity_B = criterion_identity(same_B, real_B) * 5.0
            same_A, modefreeA = netG_B2A(real_A)
            loss_identity_A = criterion_identity(same_A, real_A) * 5.0

            # Domain adaptation loss with gradient reversal
            features = torch.cat([modefreeA, modefreeB], dim=0)
            label_A = torch.zeros(batch_size, dtype=torch.long, device=device)
            label_B = torch.ones(batch_size, dtype=torch.long, device=device)
            labels = torch.cat([label_A, label_B], dim=0)

            permutation = torch.randperm(features.size(0), device=device)
            features_shuffled = features[permutation]
            labels_shuffled = labels[permutation]
            # æ¢¯åº¦åè½¬ï¼šè®©ç”Ÿæˆå™¨å­¦ä¹ ç”ŸæˆåŸŸä¸å˜çš„ç‰¹å¾
            p = epoch / opt.epoch
            lamba = 2 / (1 + math.exp(-5 * p)) - 1
            features_grl = GradientReversalLayer.apply(features_shuffled, lamba)
            pred_shuffled = domain_cls(features_grl)
            loss_domain = criterion_domain(pred_shuffled, labels_shuffled) * 5

            # GAN loss
            fake_B, shared_real_A = netG_A2B(real_A)

            # Get discriminator instance (handles DDP)
            netD_B_instance = netD_B.module if world_size > 1 else netD_B
            pred_fake = netD_B_instance(fake_B)
            loss_GAN_A2B = criterion_GAN(pred_fake, target_real)

            fake_A, shared_real_B = netG_B2A(real_B)
            netD_A_instance = netD_A.module if world_size > 1 else netD_A
            pred_fake = netD_A_instance(fake_A)
            loss_GAN_B2A = criterion_GAN(pred_fake, target_real)

            # Cycle loss
            recovered_A, shared_fake_B = netG_B2A(fake_B)
            loss_cycle_ABA = criterion_cycle(recovered_A, real_A) * 10.0

            recovered_B, shared_fake_A = netG_A2B(fake_A)
            loss_cycle_BAB = criterion_cycle(recovered_B, real_B) * 10.0

            # Content consistency loss
            loss_consistency_A = criterion_identity(shared_real_A, shared_fake_B) * 5.0
            loss_consistency_B = criterion_identity(shared_real_B, shared_fake_A) * 5.0

            # Total loss
            loss_G = (loss_identity_A + loss_identity_B + loss_GAN_A2B +
                      loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB +
                      loss_consistency_A + loss_consistency_B + loss_domain)

            # å¯¹æŸå¤±è¿›è¡Œå½’ä¸€åŒ–ï¼Œå› ä¸ºæˆ‘ä»¬è¦ç´¯ç§¯æ¢¯åº¦
            loss_G = loss_G / accumulation_steps
            loss_G.backward()

            # åªåœ¨ç´¯ç§¯æ­¥éª¤ç»“æŸæ—¶æ›´æ–°å‚æ•°
            if (i + 1) % accumulation_steps == 0:
                optimizer_G.step()

            ###### Discriminator A ######
            # åªåœ¨ç´¯ç§¯æ­¥éª¤çš„å¼€å§‹æ—¶æ¸…é›¶æ¢¯åº¦
            if i % accumulation_steps == 0:
                optimizer_D_A.zero_grad()

            # Real loss
            pred_real = netD_A_instance(real_A)
            loss_D_real = criterion_GAN(pred_real, target_real)

            # Fake loss
            fake_A = fake_A_buffer.push_and_pop(fake_A)
            pred_fake = netD_A_instance(fake_A.detach())
            loss_D_fake = criterion_GAN(pred_fake, target_fake)

            # Total loss
            loss_D_A = (loss_D_real + loss_D_fake) * 0.5
            # å¯¹æŸå¤±è¿›è¡Œå½’ä¸€åŒ–
            loss_D_A = loss_D_A / accumulation_steps
            loss_D_A.backward()

            # åªåœ¨ç´¯ç§¯æ­¥éª¤ç»“æŸæ—¶æ›´æ–°å‚æ•°
            if (i + 1) % accumulation_steps == 0:
                optimizer_D_A.step()

            ###### Discriminator B ######
            # åªåœ¨ç´¯ç§¯æ­¥éª¤çš„å¼€å§‹æ—¶æ¸…é›¶æ¢¯åº¦
            if i % accumulation_steps == 0:
                optimizer_D_B.zero_grad()

            # Real loss
            pred_real = netD_B_instance(real_B)
            loss_D_real = criterion_GAN(pred_real, target_real)

            # Fake loss
            fake_B = fake_B_buffer.push_and_pop(fake_B)
            pred_fake = netD_B_instance(fake_B.detach())
            loss_D_fake = criterion_GAN(pred_fake, target_fake)

            # Total loss
            loss_D_B = (loss_D_real + loss_D_fake) * 0.5
            # å¯¹æŸå¤±è¿›è¡Œå½’ä¸€åŒ–
            loss_D_B = loss_D_B / accumulation_steps
            loss_D_B.backward()

            # åªåœ¨ç´¯ç§¯æ­¥éª¤ç»“æŸæ—¶æ›´æ–°å‚æ•°
            if (i + 1) % accumulation_steps == 0:
                optimizer_D_B.step()

            ###### Domain Classifier ######
            # åªåœ¨ç´¯ç§¯æ­¥éª¤çš„å¼€å§‹æ—¶æ¸…é›¶æ¢¯åº¦
            if i % accumulation_steps == 0:
                optimizer_domain.zero_grad()

            # æ­£å¸¸è®­ç»ƒåŸŸåˆ†ç±»å™¨ï¼ˆä¸åè½¬æ¢¯åº¦ï¼‰
            pred_normal = domain_cls(features_shuffled.detach())
            loss_domain_normal = criterion_domain(pred_normal, labels_shuffled)
            # å¯¹æŸå¤±è¿›è¡Œå½’ä¸€åŒ–
            loss_domain_normal = loss_domain_normal / accumulation_steps
            loss_domain_normal.backward()

            # åªåœ¨ç´¯ç§¯æ­¥éª¤ç»“æŸæ—¶æ›´æ–°å‚æ•°
            if (i + 1) % accumulation_steps == 0:
                optimizer_domain.step()

            if rank == 0 and i % 200 == 0:
                domain_acc = (pred_shuffled.argmax(dim=1) == labels_shuffled).float().mean()
                print(f'[Epoch {epoch}] [batch {i}] '
                      f'Domain Loss: {loss_domain.item() * accumulation_steps:.4f}, Domain Acc: {domain_acc.item():.4f}',
                      f'Loss_total: {loss_G.item() * accumulation_steps:.4f}')  #

        # Update learning rates
        lr_scheduler_G.step()
        lr_scheduler_D_A.step()
        lr_scheduler_D_B.step()
        lr_scheduler_domain.step()

        # Save models checkpoints (only on master process)
        if rank == 0:
            # Handle DDP model saving
            shared_generator_model = shared_generator.module if world_size > 1 else shared_generator
            netD_A_model = netD_A.module if world_size > 1 else netD_A
            netD_B_model = netD_B.module if world_size > 1 else netD_B

            shared_generator_shared_layers_model = shared_generator.module.shared_layers if world_size > 1 else shared_generator.shared_layers

            torch.save(shared_generator_model.state_dict(), 'output/shared_generator.pth')
            torch.save(netD_A_model.state_dict(), 'output/netD_A.pth')
            torch.save(netD_B_model.state_dict(), 'output/netD_B.pth')

            torch.save(shared_generator_shared_layers_model.state_dict(), 'output/shared_generator_sharedlayers.pth')

    # ============== é‡Šæ”¾CycleGANç›¸å…³çš„GPUèµ„æº ==============
    # 1. åˆ é™¤ä¸å†ä½¿ç”¨çš„åˆ¤åˆ«å™¨ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ç­‰ï¼ˆè¿™äº›åœ¨Rectified Flowé˜¶æ®µä¸éœ€è¦ï¼‰
    del netD_A, netD_B, optimizer_D_A, optimizer_D_B
    del lr_scheduler_D_A, lr_scheduler_D_B, criterion_GAN, criterion_cycle, criterion_identity
    # 2. åˆ é™¤è¾“å…¥ç¼“å­˜å’Œç¼“å†²åŒºï¼ˆCycleGANç‰¹æœ‰çš„ä¸­é—´å˜é‡ï¼‰
    del target_real, target_fake, fake_A_buffer, fake_B_buffer
    # 3. å¼ºåˆ¶Pythonåƒåœ¾å›æ”¶ï¼ˆåˆ é™¤å¼•ç”¨è®¡æ•°ä¸º0çš„å¯¹è±¡ï¼‰
    import gc
    gc.collect()
    # 4. æ¸…ç©ºCUDAæœªä½¿ç”¨çš„ç¼“å­˜ï¼ˆé‡Šæ”¾GPUæ˜¾å­˜ï¼‰
    torch.cuda.empty_cache()

    # Train Rectified Flow

    # checkpoint_path = 'output/shared_generator_deep.pth'
    # checkpoint = torch.load(checkpoint_path, map_location=device)
    # # æå–state_dictï¼ˆå…¼å®¹ä¸åŒä¿å­˜æ–¹å¼ï¼‰
    # state_dict = checkpoint.get('shared_generator', checkpoint)
    #
    # # åˆ¤æ–­å½“å‰æ¨¡å‹æ˜¯å¦è¢«DDPåŒ…è£…
    # is_ddp_model = isinstance(shared_generator, torch.nn.parallel.DistributedDataParallel)
    #
    # # å¤„ç†å‰ç¼€ä¸åŒ¹é…é—®é¢˜
    # if is_ddp_model:
    #     # æ¨¡å‹æ˜¯DDPçš„ï¼ˆéœ€è¦å¸¦module.å‰ç¼€ï¼‰ï¼Œå¦‚æœstate_dictæ²¡æœ‰å‰ç¼€ï¼Œåˆ™æ·»åŠ 
    #     if not all(key.startswith('module.') for key in state_dict.keys()):
    #         state_dict = {f"module.{k}": v for k, v in state_dict.items()}
    # else:
    #     # æ¨¡å‹ä¸æ˜¯DDPçš„ï¼ˆä¸éœ€è¦module.å‰ç¼€ï¼‰ï¼Œå¦‚æœstate_dictæœ‰å‰ç¼€ï¼Œåˆ™ç§»é™¤
    #     if all(key.startswith('module.') for key in state_dict.keys()):
    #         state_dict = {k[7:]: v for k, v in state_dict.items()}
    #
    # # åŠ è½½æƒé‡
    # shared_generator.load_state_dict(state_dict, strict=True)
    # print(f"æˆåŠŸä» {checkpoint_path} åŠ è½½SharedGeneratorå‚æ•°")
    #
    # print(f"Successfully load SharedGenerator from {checkpoint_path} for Rectified Flow training")
    # # Freeze SharedGenerator
    # for param in shared_generator.parameters():
    #     param.requires_grad = False

    # Load Rectified Flow config
    config = './config/train_config.yaml'
    config = yaml.load(open(config, 'rb'), Loader=yaml.FullLoader)
    base_channels = config.get('base_channels', 256)
    # epochs = config.get('epochs', 10)
    # batch_size = config.get('batch_size', 64)
    lr_adjust_epoch = config.get('lr_adjust_epoch', 50)
    batch_print_interval = config.get('batch_print_interval', 100)
    # checkpoint_save_interval = config.get('checkpoint_save_interval', 1)
    save_path = config.get('save_path', './checkpoints')
    use_cfg = config.get('use_cfg', False)
    device = torch.device(f"cuda:{rank}" if torch.cuda.is_available() and opt.cuda else "cpu")

    # Create and wrap Rectified Flow model
    Unetmodel = MiniUnet(base_channels).to(device)

    # ============== æ–°å¢ï¼šè®¡ç®—å¹¶è¾“å‡ºMiniUnetå‚æ•°é‡ ==============
    def count_parameters(model):
        """è®¡ç®—æ¨¡å‹æ€»å‚æ•°é‡å’Œå¯è®­ç»ƒå‚æ•°é‡"""
        total_params = sum(p.numel() for p in model.parameters())  # æ€»å‚æ•°é‡
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)  # å¯è®­ç»ƒå‚æ•°é‡
        return total_params, trainable_params

    # è®¡ç®—å‚æ•°é‡
    total_params, trainable_params = count_parameters(Unetmodel)
    # æ ¼å¼åŒ–è¾“å‡ºï¼ˆè½¬æ¢ä¸ºç™¾ä¸‡(M)å•ä½ï¼Œä¿ç•™2ä½å°æ•°ï¼‰
    print(f"MiniUnet æ¨¡å‹å‚æ•°é‡ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°é‡: {total_params:,} ä¸ª ({total_params / 1e6:.2f}M)")
    print(f"  å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,} ä¸ª ({trainable_params / 1e6:.2f}M)")
    # ==========================================================

    if world_size > 1:
        Unetmodel = DDP(Unetmodel, device_ids=[rank])

    # Optimizer and scheduler
    optimizer = optim.AdamW(Unetmodel.parameters(), lr=1e-4, weight_decay=0.1)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_adjust_epoch, gamma=0.1)

    rf = RectifiedFlow()
    loss_list = []

    # Train Rectified Flow (2)
    # Get generator instance (handles DDP)
    shared_generator_instance = shared_generator.module if world_size > 1 else shared_generator
    for epoch in range(opt.epoch, opt.n_epochs):
        if world_size > 1:
            sampler.set_epoch(epoch)
        if rank == 0:
            print(f'Rectified Flow Epoch {epoch}/{opt.n_epochs}')
            # åˆå§‹åŒ–æ¯ä¸ªepochçš„lossè®°å½•
            epoch_loss = 0.0

        for i, batch in enumerate(dataloader):
            real_A = Variable(input_A.copy_(batch['A'])).to(device)
            real_B = Variable(input_B.copy_(batch['B'])).to(device)

            fake_B, shared_real_B = shared_generator_instance.forward_a2b(real_B)

            # Prepare data for Rectified Flow
            false_batch = fake_B
            true_batch = real_B
            x_1 = true_batch
            t = torch.rand(x_1.size(0)).to(device)

            # Generate flow
            x_t, _ = rf.create_flow(x_1, t)
            x_0 = false_batch

            # Move data to device
            x_t = x_t.to(device)
            x_0 = x_0.to(device)
            x_1 = x_1.to(device)

            optimizer.zero_grad()

            if use_cfg:
                # CFG handling
                x_t = torch.cat([x_t, x_t.clone()], dim=0)
                t = torch.cat([t, t.clone()], dim=0)
                y = torch.cat([torch.ones(x_1.size(0)), -torch.ones(x_1.size(0))], dim=0)
                x_1 = torch.cat([x_1, x_1.clone()], dim=0)
                x_0 = torch.cat([x_0, x_0.clone()], dim=0)
                y = y.to(device)
            else:
                y = None

            # Forward pass
            v_pred = Unetmodel(x=x_t, t=t, y=y)

            # Calculate loss
            loss = rf.mse_loss(v_pred, x_1, x_0)
            # ç´¯åŠ æ¯ä¸ªbatchçš„loss
            if rank == 0:
                epoch_loss += loss.item()

            # Backward pass
            loss.backward()
            optimizer.step()

            # Print training info (only on master process)
            if rank == 0 and i % batch_print_interval == 0:
                print(f'[Epoch {epoch}] [batch {i}] loss: {loss.item()}')

            # Record loss (only on master process)
            if rank == 0:
                loss_list.append(loss.item())

        # æ‰“å°æ¯ä¸ªepochçš„å¹³å‡loss
        if rank == 0:
            avg_epoch_loss = epoch_loss / len(dataloader)
            print(f'[Epoch {epoch} finished] average_loss: {avg_epoch_loss:.6f}')
        # Update learning rate
        scheduler.step()

        # ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆä»…åœ¨ä¸»GPUä¸Šï¼‰
        # Save checkpoint (only on master process)
        if epoch == opt.n_epochs - 1:
            print(f'Saving model {epoch} to {save_path}...')
            save_dict = dict(
                # ä½¿ç”¨å½“å‰æ¨¡å‹çŠ¶æ€ï¼ˆå·²å¤„ç†DDPåŒ…è£…ï¼‰
                Unetmodel=Unetmodel.module.state_dict() if hasattr(Unetmodel, 'module') else Unetmodel.state_dict(),
                optimizer=optimizer.state_dict(),
                scheduler=scheduler.state_dict(),
                epoch=epoch,
                loss_list=loss_list
            )
            torch.save(save_dict, os.path.join(save_path, f'miniunet_finalepoch.pth'))

    del shared_generator
    # Cleanup distributed training
    if world_size > 1:
        cleanup()

def infer(
        checkpoint_path,
        base_channels=16,
        step=50,
        num_imgs=1,
        org_img=None,
        filenames=None,
        y=None,
        cfg_scale=7.0,
        save_path='./results',
        save_path_motaiwuguan='./results',
        save_noise_path=None,
        device='cuda',
        shared_generator=None,
        img=None,
        world_size=1):
    os.makedirs(save_path, exist_ok=True)
    if save_noise_path is not None:
        os.makedirs(save_noise_path, exist_ok=True)

    if y is not None:
        assert len(y.shape) == 1 or len(y.shape) == 2, 'y must be 1D or 2D tensor'
        assert y.shape[0] == num_imgs or y.shape[0] == 1, 'y.shape[0] must match num_imgs or be 1'
        if y.shape[0] == 1:
            y = y.repeat(num_imgs, 1).reshape(num_imgs)
        y = y.to(device)

    model = MiniUnet(base_channels=base_channels)
    model.to(device)
    model.eval()

    rf = RectifiedFlow()

    checkpoint = torch.load(checkpoint_path, map_location=device)
    state_dict = checkpoint['Unetmodel']
    if all(key.startswith('module.') for key in state_dict.keys()):
        new_state_dict = {k[7:]: v for k, v in state_dict.items()}
        model.load_state_dict(new_state_dict)
    else:
        model.load_state_dict(state_dict)

    shared_generator_instance = shared_generator.module if world_size > 1 else shared_generator
    shared_generator_instance.eval()  # ç¡®ä¿ç”Ÿæˆå™¨å¤„äºè¯„ä¼°æ¨¡å¼

    # with torch.no_grad():
    #     for i in range(num_imgs):
    #         print(f'Generating {i}th image...')
    #         dt = 1.0 / step
    #
    #         if img=='uto':
    #             # ç”Ÿæˆ fake_B å¹¶ä¿å­˜
    #             fake_org, shared_real_org = shared_generator_instance.forward_a2b(org_img.to(device))  # åŸç”Ÿæˆfake_Bçš„ä»£ç 
    #         elif img=='mri':
    #             fake_org, shared_real_org = shared_generator_instance.forward_b2a(org_img.to(device))  # åŸç”Ÿæˆfake_Bçš„ä»£ç 
    #         # ====================== æ–°å¢ï¼šä¿å­˜ fake_B ä¸ºå›¾ç‰‡ ======================
    #         # 1. å–ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆç§»é™¤batchç»´åº¦ï¼‰
    #         fake_org_single = fake_org[0]  # å½¢çŠ¶: [C, H, W]
    #
    #         # 2. åå½’ä¸€åŒ–ï¼ˆå’Œx_tå¤„ç†ä¸€è‡´ï¼‰
    #         mean = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1).to(device)
    #         std = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1).to(device)
    #         fake_org_denorm = fake_org_single * std + mean  # åå½’ä¸€åŒ–
    #
    #         # 3. çº¿æ€§ç¼©æ”¾åˆ°[0, 1]ï¼ˆä¿ç•™çº¹ç†ï¼Œä¸æˆªæ–­ï¼‰
    #         # å±•å¹³è®¡ç®—min/maxï¼ˆä¸ºå•å¼ å›¾ç‰‡æ·»åŠ batchç»´åº¦ä¾¿äºå¤ç”¨ä»£ç ï¼‰
    #         fake_org_flat = fake_org_denorm.unsqueeze(0).view(1, 3, -1)  # å½¢çŠ¶: [1, 3, H*W]
    #         min_val = fake_org_flat.min(dim=2, keepdim=True)[0].view(1, 3, 1, 1)
    #         max_val = fake_org_flat.max(dim=2, keepdim=True)[0].view(1, 3, 1, 1)
    #         range_val = max_val - min_val
    #         range_val[range_val == 0] = 1.0  # é¿å…é™¤é›¶
    #         fake_org_01 = (fake_org_denorm - min_val[0]) / range_val[0]  # ç¼©æ”¾åˆ°[0,1]
    #
    #         # 4. è½¬æ¢ä¸ºOpenCVæ ¼å¼å¹¶ä¿å­˜
    #         fake_org_np = fake_org_01.detach().cpu().numpy()  # [C, H, W]
    #         fake_org_np = fake_org_np.transpose(1, 2, 0)  # è½¬æ¢ä¸º[H, W, C]
    #         fake_org_np = cv2.cvtColor(fake_org_np, cv2.COLOR_RGB2BGR)  # è½¬æ¢ä¸ºBGRï¼ˆOpenCVé»˜è®¤æ ¼å¼ï¼‰
    #         fake_org_np = (fake_org_np * 255).astype('uint8')  # è½¬æ¢ä¸º0-255çš„uint8
    #
    #         # 5. ä¿å­˜ï¼ˆæ–‡ä»¶åæ·»åŠ "_fake_B"åç¼€åŒºåˆ†ï¼‰
    #         if img == 'uto':
    #             fake_org_filename = f'{os.path.splitext(filenames[0])[0]}.png'  #2Mri
    #             print(f"save transfered images by rectified flow further, not here")
    #         elif img == 'mri':
    #             fake_org_filename = f'{os.path.splitext(filenames[0])[0]}.png'  #2Uto
    #             cv2.imwrite(os.path.join(save_path, fake_org_filename), fake_org_np)
    #             print(f"å·²ä¿å­˜ fake_B è‡³: {os.path.join(save_path, fake_org_filename)}")
    #         # ==================================================================
    #
    #         ## +++++++++++++++++ä¿å­˜æ¨¡æ€æ— å…³ç‰¹å¾å›¾++++++++++++++++++++++++++++++++++++++++++++++++++++
    #         # 1. å–ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆç§»é™¤batchç»´åº¦ï¼‰ï¼Œå½¢çŠ¶ä¸º [32, 256, 256]
    #         fake_org_single = (shared_real_org[0] + 1) / 2  # å½¢çŠ¶: [32, 256, 256]
    #
    #         # 2. åˆ›å»ºå¯è§†åŒ–æ–¹æ¡ˆï¼šå°†32ä¸ªé€šé“æ’åˆ—æˆç½‘æ ¼
    #         def visualize_32ch_as_grid(feature_maps, grid_cols=8):
    #             """
    #             å°†32é€šé“çš„ç‰¹å¾å›¾æ’åˆ—æˆç½‘æ ¼è¿›è¡Œå¯è§†åŒ–
    #             feature_maps: [32, H, W] å¼ é‡
    #             grid_cols: ç½‘æ ¼åˆ—æ•°
    #             """
    #             channels, H, W = feature_maps.shape
    #             grid_rows = (channels + grid_cols - 1) // grid_cols  # è®¡ç®—éœ€è¦çš„è¡Œæ•°
    #
    #             # åˆ›å»ºç©ºç™½ç”»å¸ƒ
    #             grid_h = grid_rows * H
    #             grid_w = grid_cols * W
    #             grid_image = torch.zeros((3, grid_h, grid_w), device=feature_maps.device)
    #
    #             # å°†æ¯ä¸ªé€šé“çš„ç‰¹å¾å›¾æ”¾ç½®åˆ°ç½‘æ ¼ä¸­
    #             for ch_idx in range(channels):
    #                 row = ch_idx // grid_cols
    #                 col = ch_idx % grid_cols
    #
    #                 # è·å–å½“å‰é€šé“çš„ç‰¹å¾å›¾
    #                 single_channel = feature_maps[ch_idx]  # [H, W]
    #
    #                 # å¯¹å•ä¸ªé€šé“è¿›è¡Œå½’ä¸€åŒ–åˆ°[0,1]
    #                 ch_min = single_channel.min()
    #                 ch_max = single_channel.max()
    #                 if ch_max - ch_min > 1e-6:
    #                     normalized_ch = (single_channel - ch_min) / (ch_max - ch_min)
    #                 else:
    #                     normalized_ch = torch.zeros_like(single_channel)
    #
    #                 # å°†å•é€šé“å¤åˆ¶åˆ°RGBä¸‰ä¸ªé€šé“ï¼ˆåˆ›å»ºç°åº¦å›¾ï¼‰
    #                 rgb_channel = normalized_ch.unsqueeze(0).repeat(3, 1, 1)  # [3, H, W]
    #
    #                 # æ”¾ç½®åˆ°ç½‘æ ¼å¯¹åº”ä½ç½®
    #                 start_h = row * H
    #                 start_w = col * W
    #                 grid_image[:, start_h:start_h + H, start_w:start_w + W] = rgb_channel
    #
    #             return grid_image
    #
    #         # 3. ç”Ÿæˆç½‘æ ¼å¯è§†åŒ–
    #         grid_visualization = visualize_32ch_as_grid(fake_org_single, grid_cols=8)
    #
    #         # 4. è½¬æ¢ä¸ºOpenCVæ ¼å¼å¹¶ä¿å­˜
    #         def save_grid_visualization(grid_tensor, save_path, filename):
    #             """ä¿å­˜ç½‘æ ¼å¯è§†åŒ–ç»“æœ"""
    #             # è½¬æ¢ä¸ºnumpy
    #             grid_np = grid_tensor.detach().cpu().numpy()  # [3, grid_h, grid_w]
    #             grid_np = grid_np.transpose(1, 2, 0)  # è½¬æ¢ä¸º [grid_h, grid_w, 3]
    #
    #             # è½¬æ¢ä¸ºBGRå¹¶ç¼©æ”¾åˆ°0-255
    #             grid_np = cv2.cvtColor(grid_np, cv2.COLOR_RGB2BGR)
    #             grid_np = (grid_np * 255).astype(np.uint8)
    #
    #             # ä¿å­˜
    #             os.makedirs(save_path, exist_ok=True)
    #             save_full_path = os.path.join(save_path, filename)
    #             cv2.imwrite(save_full_path, grid_np)
    #
    #             return save_full_path, grid_np.shape
    #
    #         # 5. ä¿å­˜ç½‘æ ¼å›¾
    #         filename = f'{os.path.splitext(filenames[0])[0]}_32ch_grid.png'
    #         save_path, final_shape = save_grid_visualization(
    #             grid_visualization,
    #             save_path_motaiwuguan,
    #             filename
    #         )
    #         print(f"âœ… å·²ä¿å­˜32é€šé“ç½‘æ ¼å¯è§†åŒ–å›¾è‡³ï¼š{save_path}")
    #         print(f"ğŸ“ æœ€ç»ˆå›¾åƒå°ºå¯¸ï¼š{final_shape[1]}Ã—{final_shape[0]} (å®½Ã—é«˜)")
    #         print(f"ğŸ”¢ é€šé“æ’åˆ—ï¼š4è¡ŒÃ—8åˆ— (å…±32ä¸ªé€šé“)")
    #
    #         def create_pca_summary(feature_maps, save_path, filename):
    #             """ä½¿ç”¨PCAå°†32é€šé“é™ç»´ä¸º3é€šé“å½©è‰²å›¾"""
    #             # é‡å¡‘æ•°æ®: [32, H*W] -> [H*W, 32]
    #             features_flat = feature_maps.view(opt.mtwg_fea, -1).T.detach().cpu().numpy()  # [H*W, 32]
    #
    #             # åº”ç”¨PCA
    #             from sklearn.decomposition import PCA
    #             pca = PCA(n_components=3)
    #             pca_result = pca.fit_transform(features_flat)  # [H*W, 3]
    #
    #             # å½’ä¸€åŒ–åˆ°[0,1]
    #             pca_min = pca_result.min(axis=0)
    #             pca_max = pca_result.max(axis=0)
    #             pca_normalized = (pca_result - pca_min) / (pca_max - pca_min + 1e-8)
    #
    #             # é‡å¡‘å›å›¾åƒå°ºå¯¸
    #             H, W = feature_maps.shape[1], feature_maps.shape[2]
    #             pca_image = pca_normalized.reshape(H, W, 3)
    #
    #             # è½¬æ¢ä¸ºBGRå¹¶ä¿å­˜
    #             pca_image_bgr = cv2.cvtColor((pca_image * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)
    #             pca_filename = f'{filename}_pca_summary.png'
    #             cv2.imwrite(os.path.join(save_path, pca_filename), pca_image_bgr)
    #
    #             print(f"âœ… å·²ä¿å­˜PCAæ‘˜è¦å›¾è‡³ï¼š{os.path.join(save_path, pca_filename)}")
    #             print(f"ğŸ“Š PCAè§£é‡Šæ–¹å·®æ¯”ï¼š{pca.explained_variance_ratio_}")
    #
    #         # å¯é€‰ï¼šå–æ¶ˆæ³¨é‡Šä»¥ç”ŸæˆPCAæ‘˜è¦å›¾
    #         create_pca_summary(fake_org_single, save_path_motaiwuguan,
    #                           os.path.splitext(filenames[0])[0])
    #
    #         if opt.epoch != opt.n_epochs and img == 'uto':
    #         # not training rectified flow, and do not generate images by it.
    #             # ç»§ç»­å¤„ç†åç»­ç”Ÿæˆæ­¥éª¤ï¼ˆåŸä»£ç é€»è¾‘ï¼‰
    #             x_t = fake_org  # ç”¨fake_Bä½œä¸ºåˆå§‹x_t
    #             noise = x_t.detach().cpu().numpy()
    #
    #             if y is not None:
    #                 y_i = y[i].unsqueeze(0)
    #
    #             for j in range(step):
    #                 if j % 10 == 0:
    #                     print(f'Generating {i}th image, step {j}...')
    #                 t = j * dt
    #                 t = torch.tensor([t]).to(device)
    #
    #                 if y is not None:
    #                     v_pred_uncond = model(x=x_t, t=t)
    #                     v_pred_cond = model(x=x_t, t=t, y=y_i)
    #                     v_pred = v_pred_uncond + cfg_scale * (v_pred_cond - v_pred_uncond)
    #                 else:
    #                     v_pred = model(x=x_t, t=t)
    #
    #                 x_t = rf.euler(x_t, v_pred, dt)
    #
    #             # å¤„ç†å¹¶ä¿å­˜æœ€ç»ˆç”Ÿæˆçš„x_tï¼ˆåŸä»£ç é€»è¾‘ï¼Œä¿æŒä¸å˜ï¼‰
    #             x_t = x_t[0]
    #             filename = filenames[0]
    #
    #             mean = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1).to(device)
    #             std = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1).to(device)
    #             x_t = x_t * std + mean
    #             batch_size, channels, h, w = x_t.unsqueeze(0).shape
    #             x_flat = x_t.view(batch_size, channels, -1)
    #             min_val = x_flat.min(dim=2, keepdim=True)[0].view(batch_size, channels, 1, 1)
    #             max_val = x_flat.max(dim=2, keepdim=True)[0].view(batch_size, channels, 1, 1)
    #             range_val = max_val - min_val
    #             range_val[range_val == 0] = 1.0
    #             x_t_01 = (x_t - min_val) / range_val
    #             img = x_t_01.detach().cpu().numpy()
    #
    #             img = img.squeeze().transpose(1, 2, 0)  # è½¬æ¢ä¸º[H, W, C]
    #             img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)  # è½¬æ¢ä¸ºBGR
    #             img = (img * 255).astype('uint8')
    #             cv2.imwrite(os.path.join(save_path, f'{filename}.png'), img)
    #             if save_noise_path is not None:
    #                 np.save(os.path.join(save_noise_path, f'{i}.npy'), noise)
    with torch.no_grad():
        for i in range(num_imgs):
            print(f'Generating {i}th image...')
            dt = 1.0 / step

            if img == 'uto':
                # ç”Ÿæˆ fake_B å¹¶ä¿å­˜
                fake_org, shared_real_org = shared_generator_instance.forward_a2b(org_img.to(device))
            elif img == 'mri':
                fake_org, shared_real_org = shared_generator_instance.forward_b2a(org_img.to(device))

            # ====================== å¢å¼ºå¯¹æ¯”åº¦ä¼˜åŒ–ç‰ˆï¼šä¿å­˜ fake_B ä¸ºå›¾ç‰‡ ======================
            # 1. å–ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆç§»é™¤batchç»´åº¦ï¼‰
            fake_org_single = fake_org[0]  # å½¢çŠ¶: [C, H, W]

            # 2. åå½’ä¸€åŒ–ï¼ˆå’Œx_tå¤„ç†ä¸€è‡´ï¼‰
            mean = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1).to(device)
            std = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1).to(device)
            fake_org_denorm = fake_org_single * std + mean  # åå½’ä¸€åŒ–

            # æ‰“å°åŸå§‹æ•°æ®ç»Ÿè®¡
            print(f"ğŸ“Š åŸå§‹æ•°æ®ç»Ÿè®¡:")
            print(f"  æœ€å°å€¼: {fake_org_denorm.min().item():.6f}")
            print(f"  æœ€å¤§å€¼: {fake_org_denorm.max().item():.6f}")
            print(f"  å‡å€¼: {fake_org_denorm.mean().item():.6f}")

            # 3. æ™ºèƒ½å¯¹æ¯”åº¦å¢å¼ºçš„çº¿æ€§ç¼©æ”¾
            def smart_contrast_stretch(image_tensor, percentile_low=2, percentile_high=98):
                """æ™ºèƒ½å¯¹æ¯”åº¦æ‹‰ä¼¸ï¼Œä½¿ç”¨ç™¾åˆ†ä½æ•°é¿å…æç«¯å€¼å½±å“"""
                # è½¬æ¢ä¸ºnumpyå¤„ç†
                img_np = image_tensor.detach().cpu().numpy()

                # è®¡ç®—æ¯ä¸ªé€šé“çš„ç™¾åˆ†ä½æ•°
                channels = img_np.shape[0]
                enhanced_channels = []

                for c in range(channels):
                    channel_data = img_np[c].flatten()

                    # è®¡ç®—ç™¾åˆ†ä½æ•°
                    low_val = np.percentile(channel_data, percentile_low)
                    high_val = np.percentile(channel_data, percentile_high)

                    # å¦‚æœç™¾åˆ†ä½æ•°èŒƒå›´å¤ªå°ï¼Œä½¿ç”¨æœ€å°æœ€å¤§å€¼
                    if high_val - low_val < 0.01:
                        low_val = channel_data.min()
                        high_val = channel_data.max()

                    # çº¿æ€§æ‹‰ä¼¸åˆ°[0, 1]
                    if high_val - low_val > 0:
                        channel_stretched = np.clip((img_np[c] - low_val) / (high_val - low_val), 0, 1)
                    else:
                        channel_stretched = np.zeros_like(img_np[c])

                    enhanced_channels.append(channel_stretched)

                # åˆå¹¶é€šé“
                enhanced_np = np.stack(enhanced_channels, axis=0)
                return torch.from_numpy(enhanced_np).float().to(image_tensor.device)

            # åº”ç”¨æ™ºèƒ½å¯¹æ¯”åº¦æ‹‰ä¼¸
            fake_org_stretched = smart_contrast_stretch(fake_org_denorm, percentile_low=1, percentile_high=99)

            # 4. åº”ç”¨Gammaæ ¡æ­£è¿›ä¸€æ­¥å¢å¼ºå¯¹æ¯”åº¦
            def apply_gamma_correction(image_tensor, gamma=1.5):
                """åº”ç”¨Gammaæ ¡æ­£å¢å¼ºå¯¹æ¯”åº¦"""
                # Gammaæ ¡æ­£å…¬å¼: output = input^(1/gamma)
                # å¯¹äºgamma>1ï¼Œå¢å¼ºæš—éƒ¨ç»†èŠ‚ï¼›å¯¹äºgamma<1ï¼Œå¢å¼ºäº®éƒ¨ç»†èŠ‚
                gamma_corrected = torch.pow(image_tensor, 1.0 / gamma)
                return gamma_corrected

            fake_org_gamma = apply_gamma_correction(fake_org_stretched, gamma=1.2)

            # 5. è½¬æ¢ä¸ºOpenCVæ ¼å¼å¹¶åº”ç”¨è¿›ä¸€æ­¥å¢å¼º
            fake_org_np = fake_org_gamma.detach().cpu().numpy()  # [C, H, W]
            fake_org_np = fake_org_np.transpose(1, 2, 0)  # è½¬æ¢ä¸º[H, W, C]

            # ç¡®ä¿å€¼åœ¨[0, 1]èŒƒå›´å†…
            fake_org_np = np.clip(fake_org_np, 0, 1)

            # è½¬æ¢ä¸ºBGRå’Œ0-255
            fake_org_np = cv2.cvtColor(fake_org_np, cv2.COLOR_RGB2BGR)
            fake_org_np_8bit = (fake_org_np * 255).astype('uint8')

            # 6. åº”ç”¨OpenCVå¯¹æ¯”åº¦å¢å¼ºæŠ€æœ¯
            def enhance_contrast_opencv(image):
                """ä½¿ç”¨OpenCVæŠ€æœ¯å¢å¼ºå›¾åƒå¯¹æ¯”åº¦"""
                if len(image.shape) == 3 and image.shape[2] == 3:
                    # æ–¹æ³•1: CLAHEï¼ˆå¯¹æ¯”åº¦å—é™çš„è‡ªé€‚åº”ç›´æ–¹å›¾å‡è¡¡åŒ–ï¼‰
                    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
                    l, a, b = cv2.split(lab)

                    # åº”ç”¨CLAHEåˆ°Lé€šé“
                    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
                    l_enhanced = clahe.apply(l)

                    # åˆå¹¶é€šé“å¹¶è½¬æ¢å›BGR
                    lab_enhanced = cv2.merge([l_enhanced, a, b])
                    enhanced = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)

                    # æ–¹æ³•2: ç›´æ–¹å›¾å‡è¡¡åŒ–ï¼ˆYCrCbç©ºé—´ï¼‰
                    ycrcb = cv2.cvtColor(enhanced, cv2.COLOR_BGR2YCrCb)
                    y, cr, cb = cv2.split(ycrcb)
                    y_eq = cv2.equalizeHist(y)
                    ycrcb_eq = cv2.merge([y_eq, cr, cb])
                    enhanced = cv2.cvtColor(ycrcb_eq, cv2.COLOR_YCrCb2BGR)

                    # æ–¹æ³•3: é”åŒ–æ»¤æ³¢å™¨å¢å¼ºçº¹ç†
                    kernel = np.array([[-1, -1, -1],
                                       [-1, 9, -1],
                                       [-1, -1, -1]])
                    enhanced = cv2.filter2D(enhanced, -1, kernel)

                    return enhanced
                else:
                    # ç°åº¦å›¾åƒå¤„ç†
                    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
                    enhanced = clahe.apply(image)
                    enhanced = cv2.equalizeHist(enhanced)
                    return enhanced

            # åº”ç”¨OpenCVå¯¹æ¯”åº¦å¢å¼º
            fake_org_enhanced = enhance_contrast_opencv(fake_org_np_8bit)

            # 7. ä¿å­˜å›¾åƒ
            if img == 'uto':
                fake_org_filename = f'{os.path.splitext(filenames[0])[0]}_enhanced.png'
                print(f"ä¿å­˜è½¬æ¢åçš„å›¾åƒï¼ˆä½¿ç”¨æ ¡æ­£æµè¿›ä¸€æ­¥å¤„ç†ï¼‰")
            elif img == 'mri':
                fake_org_filename = f'{os.path.splitext(filenames[0])[0]}_enhanced.png'

                # ä¿å­˜å¢å¼ºç‰ˆæœ¬
                cv2.imwrite(os.path.join(save_path, fake_org_filename), fake_org_enhanced)

                # åŒæ—¶ä¿å­˜åŸå§‹å¢å¼ºç‰ˆæœ¬ç”¨äºå¯¹æ¯”
                cv2.imwrite(os.path.join(save_path, f'{os.path.splitext(filenames[0])[0]}_basic.png'), fake_org_np_8bit)

                # æ‰“å°å¢å¼ºæ•ˆæœç»Ÿè®¡
                print(f"âœ… å·²ä¿å­˜å¢å¼ºç‰ˆ fake_B è‡³: {os.path.join(save_path, fake_org_filename)}")
                print(f"ğŸ¨ å¢å¼ºæ•ˆæœ:")
                print(f"  åŸå§‹èŒƒå›´: [{fake_org_np_8bit.min()}, {fake_org_np_8bit.max()}]")
                print(f"  å¢å¼ºåèŒƒå›´: [{fake_org_enhanced.min()}, {fake_org_enhanced.max()}]")
                print(f"  å¯¹æ¯”åº¦æå‡: {(fake_org_enhanced.std() / max(fake_org_np_8bit.std(), 1)):.2f}å€")

            # ==================================================================

            ## +++++++++++++++++ä¿å­˜æ¨¡æ€æ— å…³ç‰¹å¾å›¾ï¼ˆ32å¼ ç‹¬ç«‹å›¾ç‰‡ï¼‰++++++++++++++++++++++++++++++++++++++++++++++++++++
            # 1. å–ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆç§»é™¤batchç»´åº¦ï¼‰ï¼Œå½¢çŠ¶ä¸º [32, 256, 256]
            feature_map_original = shared_real_org[0]  # å½¢çŠ¶: [32, 256, 256]

            # 2. å¢å¼ºå•ä¸ªé€šé“å¯¹æ¯”åº¦çš„å‡½æ•°
            def enhance_single_channel(channel):
                """å¢å¼ºå•ä¸ªé€šé“çš„å¯¹æ¯”åº¦ï¼Œä½¿çº¹ç†æ›´æ¸…æ™°"""
                # è½¬æ¢ä¸ºnumpy
                ch_np = channel.detach().cpu().numpy()

                # è®¡ç®—ç™¾åˆ†ä½æ•°
                p1 = np.percentile(ch_np, 1)
                p99 = np.percentile(ch_np, 99)

                # å¦‚æœç™¾åˆ†ä½æ•°èŒƒå›´å¤ªå°ï¼Œä½¿ç”¨æœ€å°æœ€å¤§å€¼
                if p99 - p1 < 0.001:
                    p1 = ch_np.min()
                    p99 = ch_np.max()

                # çº¿æ€§æ‹‰ä¼¸
                if p99 - p1 > 0:
                    channel_stretched = np.clip((ch_np - p1) / (p99 - p1), 0, 1)
                else:
                    channel_stretched = np.zeros_like(ch_np)

                # Gammaæ ¡æ­£å¢å¼ºçº¹ç†
                gamma = 0.5  # å¼ºä¼½é©¬å€¼å¢å¼ºæš—éƒ¨ç»†èŠ‚
                channel_gamma = np.power(channel_stretched, gamma)

                # è½¬æ¢ä¸º8ä½å›¾åƒ
                channel_8bit = (channel_gamma * 255).astype(np.uint8)

                # ç›´æ–¹å›¾å‡è¡¡åŒ–
                channel_eq = cv2.equalizeHist(channel_8bit)

                # å±€éƒ¨å¯¹æ¯”åº¦å¢å¼ºï¼ˆCLAHEï¼‰
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                channel_clahe = clahe.apply(channel_eq)

                return channel_clahe

            # 3. ä¿å­˜32å¼ ç‹¬ç«‹é€šé“å›¾ç‰‡
            def save_individual_channels(feature_maps, save_path, filename_prefix):
                """ä¿å­˜æ‰€æœ‰é€šé“ä¸ºç‹¬ç«‹çš„å›¾ç‰‡"""
                os.makedirs(save_path, exist_ok=True)

                channels = feature_maps.shape[0]
                saved_files = []

                for ch_idx in range(channels):
                    # è·å–å½“å‰é€šé“
                    channel = feature_maps[ch_idx]  # [H, W]

                    # å¢å¼ºé€šé“å¯¹æ¯”åº¦
                    channel_enhanced = enhance_single_channel(channel)

                    # åº”ç”¨ä¼ªå½©è‰²æ˜ å°„å¢å¼ºå¯è§†æ€§ï¼ˆå¯é€‰ï¼‰
                    if False:
                        # ä½¿ç”¨JETä¼ªå½©è‰²æ˜ å°„
                        colored_ch = cv2.applyColorMap(channel_enhanced, cv2.COLORMAP_JET)
                    else:
                        # ç°åº¦å›¾åƒ
                        colored_ch = cv2.cvtColor(channel_enhanced, cv2.COLOR_GRAY2BGR)

                    # ä¿å­˜å›¾ç‰‡
                    save_filename = f'{filename_prefix}_ch{ch_idx:02d}.png'
                    save_path_full = os.path.join(save_path, save_filename)
                    cv2.imwrite(save_path_full, colored_ch)

                    saved_files.append(save_path_full)

                return saved_files

            # 4. ä¿å­˜æ‰€æœ‰32ä¸ªé€šé“
            print(f"ğŸ’¾ ä¿å­˜32ä¸ªç‹¬ç«‹é€šé“å›¾ç‰‡...")
            saved_files = save_individual_channels(feature_map_original, save_path_motaiwuguan,
                                                   os.path.splitext(filenames[0])[0])

            print(f"âœ… å·²ä¿å­˜32ä¸ªç‹¬ç«‹é€šé“å›¾ç‰‡è‡³: {save_path_motaiwuguan}")
            print(f"ğŸ“Š é€šé“ç»Ÿè®¡:")
            print(f"  æ€»é€šé“æ•°: {feature_map_original.shape[0]}")
            print(f"  å›¾åƒå°ºå¯¸: {feature_map_original.shape[1]}Ã—{feature_map_original.shape[2]}")

            # 5. å¯é€‰ï¼šç”Ÿæˆé€šé“å¯¹æ¯”åº¦æ’å
            def calculate_channel_contrast(feature_maps):
                """è®¡ç®—æ¯ä¸ªé€šé“çš„å¯¹æ¯”åº¦ï¼ˆæ ‡å‡†å·®ï¼‰"""
                contrast_scores = []
                for ch_idx in range(feature_maps.shape[0]):
                    channel = feature_maps[ch_idx]
                    # è®¡ç®—æ ‡å‡†å·®ä½œä¸ºå¯¹æ¯”åº¦æŒ‡æ ‡
                    std_dev = channel.std().item()
                    contrast_scores.append((ch_idx, std_dev))
                return contrast_scores

            if False:
                contrast_scores = calculate_channel_contrast(feature_map_original)
                contrast_scores.sort(key=lambda x: x[1], reverse=True)

                print(f"ğŸ† é€šé“å¯¹æ¯”åº¦æ’å (å‰10):")
                for rank, (ch_idx, contrast) in enumerate(contrast_scores[:10]):
                    print(f"  æ’å{rank + 1}: é€šé“{ch_idx:02d}, å¯¹æ¯”åº¦={contrast:.6f}")

            # 6. å¯é€‰ï¼šç”Ÿæˆç‰¹å¾æ‘˜è¦å›¾ï¼ˆä½¿ç”¨PCAï¼‰
            def create_pca_summary_enhanced(feature_maps, save_path, filename_prefix):
                """ä½¿ç”¨PCAå°†32é€šé“é™ç»´ä¸º3é€šé“å½©è‰²å›¾ï¼ˆå¢å¼ºç‰ˆï¼‰"""
                try:
                    from sklearn.decomposition import PCA

                    # é‡å¡‘æ•°æ®: [32, H*W] -> [H*W, 32]
                    features_flat = feature_maps.view(feature_maps.shape[0], -1).T.detach().cpu().numpy()

                    # åº”ç”¨PCA
                    pca = PCA(n_components=3)
                    pca_result = pca.fit_transform(features_flat)

                    # å½’ä¸€åŒ–åˆ°[0,1]
                    pca_min = pca_result.min(axis=0)
                    pca_max = pca_result.max(axis=0)
                    pca_normalized = (pca_result - pca_min) / (pca_max - pca_min + 1e-8)

                    # é‡å¡‘å›å›¾åƒå°ºå¯¸
                    H, W = feature_maps.shape[1], feature_maps.shape[2]
                    pca_image = pca_normalized.reshape(H, W, 3)

                    # è½¬æ¢ä¸ºBGR
                    pca_image_bgr = cv2.cvtColor((pca_image * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)

                    # åº”ç”¨å¯¹æ¯”åº¦å¢å¼º
                    lab = cv2.cvtColor(pca_image_bgr, cv2.COLOR_BGR2LAB)
                    l, a, b = cv2.split(lab)
                    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
                    l_enhanced = clahe.apply(l)
                    lab_enhanced = cv2.merge([l_enhanced, a, b])
                    pca_enhanced = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2BGR)

                    # ä¿å­˜
                    pca_filename = f'{filename_prefix}_pca_summary_enhanced.png'
                    cv2.imwrite(os.path.join(save_path, pca_filename), pca_enhanced)

                    print(f"âœ… å·²ä¿å­˜PCAæ‘˜è¦å›¾è‡³ï¼š{os.path.join(save_path, pca_filename)}")
                    print(f"ğŸ“Š PCAè§£é‡Šæ–¹å·®æ¯”ï¼š{pca.explained_variance_ratio_}")

                except Exception as e:
                    print(f"âš ï¸ PCAæ‘˜è¦å›¾ç”Ÿæˆå¤±è´¥: {e}")

            # ç”ŸæˆPCAæ‘˜è¦å›¾ï¼ˆå¯é€‰ï¼‰
            if False:
                create_pca_summary_enhanced(feature_map_original, save_path_motaiwuguan,
                                            os.path.splitext(filenames[0])[0])

            # ==================================================================

            if opt.epoch != opt.n_epochs and img == 'uto':
                # not training rectified flow, and do not generate images by it.
                # ç»§ç»­å¤„ç†åç»­ç”Ÿæˆæ­¥éª¤ï¼ˆåŸä»£ç é€»è¾‘ï¼‰
                x_t = fake_org  # ç”¨fake_Bä½œä¸ºåˆå§‹x_t
                noise = x_t.detach().cpu().numpy()

                if y is not None:
                    y_i = y[i].unsqueeze(0)

                for j in range(step):
                    if j % 10 == 0:
                        print(f'Generating {i}th image, step {j}...')
                    t = j * dt
                    t = torch.tensor([t]).to(device)

                    if y is not None:
                        v_pred_uncond = model(x=x_t, t=t)
                        v_pred_cond = model(x=x_t, t=t, y=y_i)
                        v_pred = v_pred_uncond + cfg_scale * (v_pred_cond - v_pred_uncond)
                    else:
                        v_pred = model(x=x_t, t=t)

                    x_t = rf.euler(x_t, v_pred, dt)

                # å¤„ç†å¹¶ä¿å­˜æœ€ç»ˆç”Ÿæˆçš„x_tï¼ˆä½¿ç”¨å¢å¼ºæ–¹æ³•ï¼‰
                x_t_single = x_t[0]
                filename = filenames[0]

                # åº”ç”¨ä¸å‰é¢ç›¸åŒçš„å¢å¼ºæ–¹æ³•
                mean = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1).to(device)
                std = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1).to(device)
                x_t_denorm = x_t_single * std + mean

                # æ™ºèƒ½å¯¹æ¯”åº¦æ‹‰ä¼¸
                x_t_stretched = smart_contrast_stretch(x_t_denorm, percentile_low=1, percentile_high=99)

                # Gammaæ ¡æ­£
                x_t_gamma = apply_gamma_correction(x_t_stretched, gamma=1.2)

                # è½¬æ¢ä¸ºOpenCVæ ¼å¼
                x_t_np = x_t_gamma.detach().cpu().numpy()
                x_t_np = x_t_np.transpose(1, 2, 0)
                x_t_np = np.clip(x_t_np, 0, 1)
                x_t_np = cv2.cvtColor(x_t_np, cv2.COLOR_RGB2BGR)
                x_t_np_8bit = (x_t_np * 255).astype('uint8')

                # åº”ç”¨OpenCVå¢å¼º
                x_t_enhanced = enhance_contrast_opencv(x_t_np_8bit)

                # ä¿å­˜å›¾åƒ
                cv2.imwrite(os.path.join(save_path, f'{filename}.png'), x_t_enhanced)

                if save_noise_path is not None:
                    np.save(os.path.join(save_noise_path, f'{i}.npy'), noise)

                print(f"âœ… å·²ä¿å­˜å¢å¼ºç‰ˆæ ¡æ­£æµç”Ÿæˆå›¾åƒ: {os.path.join(save_path, f'{filename}.png')}")

class SingleFolderDataset(Dataset):
    def __init__(self, image_dir, transform=None):
        self.image_dir = image_dir
        self.transform = transform
        self.image_files = [f for f in os.listdir(image_dir)
                            if os.path.isfile(os.path.join(image_dir, f))]

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_dir, self.image_files[idx])
        image = Image.open(img_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        return image, self.image_files[idx]  # è¿”å›å›¾åƒå’Œæ–‡ä»¶å


# åˆ›å»ºæ•°æ®åŠ è½½å™¨çš„å‡½æ•°
def create_single_folder_dataloader(image_dir, batch_size=1, resize=256):
    transform = transforms.Compose([
        transforms.Resize((resize, resize)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

    dataset = SingleFolderDataset(image_dir, transform=transform)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    return dataloader


def main_infer(rank, world_size):
    # ###### infer ##########
    # Set device
    device = torch.device(f"cuda:{rank}" if torch.cuda.is_available() and opt.cuda else "cpu")
    torch.cuda.set_device(device)

    image_dir = ".\datasets/test\MRI"
    transfer_dir = './datasets/Transfer/MRI2Uto/fake_SG-cycle-3WUto'
    transfer_dir_motaiwuguan = './datasets/Transfer/MRI2Uto/shallow_motaiwuguan'
    if 'uto' in image_dir.lower():
        img = 'uto'
    elif 'mri' in image_dir.lower():
        img = 'mri'
    else:
        img = None  # æˆ–æ ¹æ®éœ€æ±‚è®¾ç½®é»˜è®¤å€¼
        print("è·¯å¾„ä¸­æ—¢ä¸åŒ…å«Utoä¹Ÿä¸åŒ…å«MRI")
    dataloader = create_single_folder_dataloader(image_dir)

    shared_generator = SharedGenerator(opt.input_nc, opt.output_nc, opt.mtwg_fea, opt.size, opt.depth4vit).to(device)
    checkpoint_path = 'output/shared_generator.pth'
    checkpoint = torch.load(checkpoint_path, map_location=device)
    # å¤„ç†DDPåŒ…è£…çš„æ¨¡å‹æƒé‡ï¼ˆå¦‚æœcheckpointæ˜¯DDPä¿å­˜çš„ï¼‰
    state_dict = checkpoint.get('shared_generator', checkpoint)  # å…¼å®¹ä¸åŒä¿å­˜æ–¹å¼
    if all(key.startswith('module.') for key in state_dict.keys()):
        # ç§»é™¤DDPæ·»åŠ çš„'module.'å‰ç¼€
        state_dict = {k[7:]: v for k, v in state_dict.items()}
    # åŠ è½½æƒé‡åˆ°æ¨¡å‹
    shared_generator.load_state_dict(state_dict, strict=True)  # strict=Trueç¡®ä¿æƒé‡å®Œå…¨åŒ¹é…
    print(f"æˆåŠŸä» {checkpoint_path} åŠ è½½SharedGeneratorå‚æ•°")


    # æ‰“å°ç¬¬ä¸€æ‰¹æ•°æ®çš„å½¢çŠ¶
    for images, filenames in dataloader:
        infer(checkpoint_path='./checkpoints/v1.1-cfg/miniunet_finalepoch.pth',
              base_channels=256,
              step=2,
              num_imgs=1,
              org_img=images,
              filenames=filenames,
              y=None,  # torch.tensor(y)
              cfg_scale=5.0,
              save_path=transfer_dir,
              save_path_motaiwuguan=transfer_dir_motaiwuguan,
              device='cuda',
              shared_generator=shared_generator,
              img=img,
              world_size=world_size)


def main():
    # Determine number of available GPUs
    world_size = torch.cuda.device_count() if opt.cuda else 1
    print(f"æ£€æµ‹åˆ° {world_size} ä¸ªGPU")

    if world_size > 1:
        # Use multiprocessing for distributed training
        mp.spawn(
            main_worker,
            args=(world_size,),
            nprocs=world_size,
            join=True
        )
    else:
        # Single GPU or CPU training
        main_worker(0, world_size)
    main_infer(0, world_size)


if __name__ == '__main__':
    mp.freeze_support()
    main()

# when opt.epoch == opt.n_epochs, training of rectified flow was forbidened,
# and default parameters for rectified flow are load.
# only the generated (tranformed) images by GAN are useful, while the images generated by rectified flow
# are formalistic